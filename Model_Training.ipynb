{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aa293416f61146cdb26b64b4be622406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3de7493e555047dea5791c1e1a52412b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1dc02cea71c14e95913c2e98e542d261",
              "IPY_MODEL_77ee7bae97ba41ba90cf600693f7bb95"
            ]
          }
        },
        "3de7493e555047dea5791c1e1a52412b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1dc02cea71c14e95913c2e98e542d261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_84ac1c5d8a0d430c885e34b8e6ee8f9f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 385,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 385,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83040f577aff4310a703fbc228718ab4"
          }
        },
        "77ee7bae97ba41ba90cf600693f7bb95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bf4bd49226184ba79a8062f4b9702841",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 385/385 [00:02&lt;00:00, 148B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad2f25de11cc45119b26473fe59797f9"
          }
        },
        "84ac1c5d8a0d430c885e34b8e6ee8f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83040f577aff4310a703fbc228718ab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf4bd49226184ba79a8062f4b9702841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad2f25de11cc45119b26473fe59797f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78cc20a9572e43bbbeacac91ee406d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2641bc17d5c34821912290e683b90f36",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4b9a96e6750d4024871d6e93f23a93db",
              "IPY_MODEL_57fc8d8c641a4e61b26a6b99f826ac24"
            ]
          }
        },
        "2641bc17d5c34821912290e683b90f36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b9a96e6750d4024871d6e93f23a93db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2413c83d4a87482f870fdcfa2b643934",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c83de144bf5441c8ff97de05b4a5c3a"
          }
        },
        "57fc8d8c641a4e61b26a6b99f826ac24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_90ceae10068f4c02aad5db3c700121e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:01&lt;00:00, 121kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_baf51010e8c54132b362d2770da6366c"
          }
        },
        "2413c83d4a87482f870fdcfa2b643934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c83de144bf5441c8ff97de05b4a5c3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90ceae10068f4c02aad5db3c700121e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "baf51010e8c54132b362d2770da6366c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b13552c533ff4fc283c516957dcae14e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5a91f8399bf149ca8c4aa7ad8a4ecced",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_56e0d2a8d7024e1cb4dc26bd38aaced3",
              "IPY_MODEL_afe154e00f2d43e6a087683b70cdebb3"
            ]
          }
        },
        "5a91f8399bf149ca8c4aa7ad8a4ecced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56e0d2a8d7024e1cb4dc26bd38aaced3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eb5642be854f4268980f00f12a36fcce",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435778770,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435778770,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94a77e13ffa6418c80535763e104b390"
          }
        },
        "afe154e00f2d43e6a087683b70cdebb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9ef56073937d439fa5e9556332541ee7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:06&lt;00:00, 71.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d081a1ac339f4fe58b0a0b1c86bfffda"
          }
        },
        "eb5642be854f4268980f00f12a36fcce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94a77e13ffa6418c80535763e104b390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ef56073937d439fa5e9556332541ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d081a1ac339f4fe58b0a0b1c86bfffda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e88676fe811648a8b9e905e6af6ddaa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8ac7037a08ff491c8e9c027d604f93e4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bcc5f758eb2c41c3aed8031d37f62857",
              "IPY_MODEL_f0ad1803b69646c7a93aafb1fbf8c01f"
            ]
          }
        },
        "8ac7037a08ff491c8e9c027d604f93e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bcc5f758eb2c41c3aed8031d37f62857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bd98e7acd29e4a42b6a702fb3e53c6d7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 385,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 385,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ecad717627e4d31a61f51b5b2574f77"
          }
        },
        "f0ad1803b69646c7a93aafb1fbf8c01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fc27c4eedc424853a96903db3c781e06",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 385/385 [00:00&lt;00:00, 13.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7a20779d0e446acbd2e825c3ca6f263"
          }
        },
        "bd98e7acd29e4a42b6a702fb3e53c6d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ecad717627e4d31a61f51b5b2574f77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fc27c4eedc424853a96903db3c781e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7a20779d0e446acbd2e825c3ca6f263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cff0de2165014919a40c638dda1a133a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_af9344008f564c34a951cba1e5adddc9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c3d6c79a348e47c0b34d2b06285a8c71",
              "IPY_MODEL_5d7b51f3b74148998e5b1270c2e0d860"
            ]
          }
        },
        "af9344008f564c34a951cba1e5adddc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3d6c79a348e47c0b34d2b06285a8c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d6753e0925ed4ab691d513ed07bb8d42",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67daa58fc630407eb689ae79bced7db6"
          }
        },
        "5d7b51f3b74148998e5b1270c2e0d860": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0e31b48895e742b8b57e1b646706282e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:01&lt;00:00, 188kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce53e18e23fd42fe814216e95e7d806a"
          }
        },
        "d6753e0925ed4ab691d513ed07bb8d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67daa58fc630407eb689ae79bced7db6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e31b48895e742b8b57e1b646706282e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce53e18e23fd42fe814216e95e7d806a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06a4dcea208249fd9c0ce964e1716812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2943f6352a804a3c86b456ca2780a3d0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7a1ad279a59648fbaea1c973fe7edad8",
              "IPY_MODEL_c614654b86814e98a56d6452b18ddb02"
            ]
          }
        },
        "2943f6352a804a3c86b456ca2780a3d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a1ad279a59648fbaea1c973fe7edad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8c1094065d354031aeb43a2ab2f67bb8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9cc1ab2c34574d2e86ab1abb51be3f54"
          }
        },
        "c614654b86814e98a56d6452b18ddb02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_08a8bc2a39c14948b9b8e5d63496e7ae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 940kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7df19be9f01443698c84e4d3241ca8cd"
          }
        },
        "8c1094065d354031aeb43a2ab2f67bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9cc1ab2c34574d2e86ab1abb51be3f54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08a8bc2a39c14948b9b8e5d63496e7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7df19be9f01443698c84e4d3241ca8cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdang20/Medication_Mentioning_Tweet_Classification/blob/main/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F-ya__3An3i"
      },
      "source": [
        "Train 10 models with BERT large to make ensemble BERT\n",
        "Max_number_epochs = 4\n",
        "patience =3\n",
        "en_core_sci_lg, threshold = 0.7\n",
        "\n",
        "Training dataset: Training 2020 + Validation 2020 + Training 2018\n",
        "\n",
        "Only use prefilter for testing dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLiUbD3zAm9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aa293416f61146cdb26b64b4be622406",
            "3de7493e555047dea5791c1e1a52412b",
            "1dc02cea71c14e95913c2e98e542d261",
            "77ee7bae97ba41ba90cf600693f7bb95",
            "84ac1c5d8a0d430c885e34b8e6ee8f9f",
            "83040f577aff4310a703fbc228718ab4",
            "bf4bd49226184ba79a8062f4b9702841",
            "ad2f25de11cc45119b26473fe59797f9",
            "78cc20a9572e43bbbeacac91ee406d10",
            "2641bc17d5c34821912290e683b90f36",
            "4b9a96e6750d4024871d6e93f23a93db",
            "57fc8d8c641a4e61b26a6b99f826ac24",
            "2413c83d4a87482f870fdcfa2b643934",
            "1c83de144bf5441c8ff97de05b4a5c3a",
            "90ceae10068f4c02aad5db3c700121e8",
            "baf51010e8c54132b362d2770da6366c",
            "b13552c533ff4fc283c516957dcae14e",
            "5a91f8399bf149ca8c4aa7ad8a4ecced",
            "56e0d2a8d7024e1cb4dc26bd38aaced3",
            "afe154e00f2d43e6a087683b70cdebb3",
            "eb5642be854f4268980f00f12a36fcce",
            "94a77e13ffa6418c80535763e104b390",
            "9ef56073937d439fa5e9556332541ee7",
            "d081a1ac339f4fe58b0a0b1c86bfffda"
          ]
        },
        "outputId": "47756138-b3ee-4dd2-ba75-9872eddbca6d"
      },
      "source": [
        "\n",
        "!pip3 install scispacy\n",
        "!pip3 install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
        "import spacy\n",
        "import scispacy\n",
        "import en_core_sci_lg\n",
        "from scispacy.umls_linking import UmlsEntityLinker\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "\n",
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification,AdamW,BertConfig,get_linear_schedule_with_warmup\n",
        "#from transformers import BertTokenizer,BertForSequenceClassification,AdamW,BertConfig,get_linear_schedule_with_warmup\n",
        "import configparser,os,time,random,pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "learning_rate=0.00001\n",
        "max_number_of_epochs=4\n",
        "checkpoint_dir = \"gdrive/My Drive/Colab Notebooks/bio-clinical-bert/\"\n",
        "gradient_clipping=1\n",
        "patience=3\n",
        "batch_size=16\n",
        "tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "device=torch.device('cuda')\n",
        "\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
        "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
        "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
        "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
        "                   \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
        "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
        "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
        "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
        "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
        "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
        "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
        "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
        "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
        "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
        "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
        "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
        "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
        "                   \"this's\": \"this is\",\n",
        "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
        "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
        "                       \"here's\": \"here is\",\n",
        "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
        "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
        "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
        "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
        "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
        "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
        "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
        "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
        "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
        "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
        "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
        "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
        "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
        "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"da\": \"the\", \"w/o\": \"without\",\"w/\": \"with\"}\n",
        "\n",
        "def camel_case_split(sent):\n",
        "  list_str = []\n",
        "  for word in sent.split(' '):\n",
        "    if word and word[0]=='#':\n",
        "      regex = r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))'\n",
        "      if re.search(regex, word):\n",
        "        camel_case = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', word)\n",
        "        for camel_case_element in camel_case:\n",
        "          list_str.append(camel_case_element)\n",
        "    else:\n",
        "        list_str.append(word)\n",
        "  sent = ' '.join(list_str)\n",
        "  return sent  \n",
        "\n",
        "def textprocess(data_path,Train=True):\n",
        "  textprocess_list = []\n",
        "  with open(data_path,'rt',encoding='utf8') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      items = line.strip().split('\\t')\n",
        "      items[2] = re.sub('http://\\S+|https://\\S+', '', items[2])\n",
        "      items[2] = re.sub(' rt[ ]@[\\S]+','',items[2]) #remove Retweet @[Name]\n",
        "      items[2] = re.sub('@[\\S]+','',items[2]) # Remove @[Name]\n",
        "      items[2] = re.sub('_[\\S]?','',items[2]) #remove underscore\n",
        "      items[2] = re.sub('&amp;?','and',items[2]) #replace '&amp;' by 'and'\n",
        "      items[2] = re.sub('&lt;','<',items[2]) #replace '&lt;' by '<'\n",
        "      items[2] = re.sub('&gt;','>',items[2]) #replace '&gt;' by '>'\n",
        "      items[2] = [''.join([i if ord(i) < 128 else '' for i in text]) for text in items[2]] #Remove non-ascii words or characters\n",
        "      items[2] = ''.join([str(elem) for elem in items[2]])\n",
        "      items[2]= re.sub('([.,!?()])', r' \\1 ',items[2]) #add a space between a word and a punctuation.\n",
        "      items[2] = re.sub('\\s{2,}', ' ', items[2]) #add a space between a word and a punctuation.\n",
        "      items[2] = camel_case_split(items[2])\n",
        "      items[2] = re.sub('#','',items[2]) #remove '#' tagging\n",
        "      items[2] = items[2].lower() #lowercase\n",
        "      items[2] = items[2].strip() #remove whitespaces at the begining and the end of tweet\n",
        "      items[2] = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in items[2].split(' ')])\n",
        "      if Train:\n",
        "        textprocess_list.append([items[2],items[4]])\n",
        "      else:\n",
        "        textprocess_list.append([items[0],items[2],items[4]])\n",
        "  return textprocess_list\n",
        "\n",
        "def mednorm(df,Train=True):\n",
        "  nlp = en_core_sci_lg.load()\n",
        "  linker = UmlsEntityLinker(threshold=0.7)\n",
        "  nlp.add_pipe(linker)\n",
        "  mednorm_list = []\n",
        "  for row in df.itertuples(index=False):\n",
        "    if Train:\n",
        "      doc = nlp(row[0])\n",
        "    else:\n",
        "      doc = nlp(row[1])  \n",
        "    has_umls = False\n",
        "    for entity in doc.ents:\n",
        "      if len(entity._.umls_ents) != 0:\n",
        "        has_umls = True\n",
        "        break\n",
        "    if has_umls:\n",
        "      if Train:\n",
        "        mednorm_list.append([row[0],row[1]])\n",
        "      else:  \n",
        "        mednorm_list.append([row[0],row[1],row[2]])\n",
        "  return mednorm_list\n",
        "\n",
        "def preprocess(df):\n",
        "  df.encoded_tokens = [tokenizer.encode_plus(text,add_special_tokens=True)['input_ids'] for text in df['tweet']] #encoded tokens for each tweet\n",
        "  df.attention_mask = [tokenizer.encode_plus(text,add_special_tokens=True)['attention_mask'] for text in df['tweet']]\n",
        "  labels=list(df['label'])\n",
        "  encoded_tokens = list(df.encoded_tokens)\n",
        "  attention_mask = list(df.attention_mask)\n",
        "  return encoded_tokens,labels,attention_mask\n",
        "\n",
        "def label_indexing(labels_train,labels_test):\n",
        "\n",
        "    labels = set(labels_train + labels_test)\n",
        "    label_to_indices,indices_to_label = {},{}\n",
        "\n",
        "    for i,label in enumerate(sorted(labels)):\n",
        "        label_to_indices[label] = i\n",
        "    for key,item in label_to_indices.items():\n",
        "        indices_to_label[item] = key\n",
        "\n",
        "    encoded_labels_train = [label_to_indices[label] for label in labels_train]\n",
        "    encoded_labels_test = [label_to_indices[label] for label in labels_test]\n",
        "\n",
        "    label_size = len(label_to_indices.keys())\n",
        "\n",
        "    return encoded_labels_train,encoded_labels_test,indices_to_label,label_size\n",
        "\n",
        "# Convert indices to Torch tensor and dump into cuda\n",
        "def feed_generator(encoded_tokens,encoded_labels,attention_mask,Training=False,output=False):\n",
        "\n",
        "    batch_size = 16\n",
        "    batch_seq = [x for x in range(int(len(encoded_tokens)/batch_size))]\n",
        "\n",
        "    if Training:\n",
        "        tmp = list(zip(encoded_tokens,encoded_labels,attention_mask))\n",
        "        random.shuffle(tmp)\n",
        "        shuffled_encoded_tokens,shuffled_encoded_labels,shuffled_attention_mask = zip(*tmp)\n",
        "    else:\n",
        "        shuffled_encoded_tokens,shuffled_encoded_labels,shuffled_attention_mask = encoded_tokens,encoded_labels,attention_mask\n",
        "        if output:\n",
        "            res = len(encoded_tokens)%batch_size\n",
        "            if res != 0:\n",
        "                batch_seq = [x for x in range(int(len(encoded_tokens)/batch_size)+1)]\n",
        "            shuffled_encoded_tokens = shuffled_encoded_tokens+shuffled_encoded_tokens[:res]\n",
        "            shuffled_encoded_labels = shuffled_encoded_labels+shuffled_encoded_labels[:res]\n",
        "            shuffled_attention_mask = shuffled_attention_mask+shuffled_attention_mask[:res]\n",
        "\n",
        "    for batch in batch_seq:\n",
        "        maxlen_sent = max([len(i) for i in shuffled_encoded_tokens[batch*batch_size:(batch+1)*batch_size]])\n",
        "        token_tensor = torch.tensor([tokens+[0]*(maxlen_sent-len(tokens)) for tokens in shuffled_encoded_tokens[batch*batch_size:(batch+1)*batch_size]])\n",
        "        attention_mask = torch.tensor([tokens+[0]*(maxlen_sent-len(tokens)) for tokens in shuffled_attention_mask[batch*batch_size:(batch+1)*batch_size]]) \n",
        "        label_tensor = torch.tensor(shuffled_encoded_labels[batch*batch_size:(batch+1)*batch_size])\n",
        "\n",
        "        token_tensor = token_tensor.to('cuda')\n",
        "        label_tensor = label_tensor.to('cuda')\n",
        "        attention_mask = attention_mask.to('cuda')\n",
        "\n",
        "        yield token_tensor,label_tensor,attention_mask\n",
        "\n",
        "def predict(model,data,output=False):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    batch_count = 0\n",
        "    for token_tensor, _, attention_mask in data:\n",
        "        with torch.no_grad():\n",
        "            logits = model(token_tensor,token_type_ids=None,attention_mask=attention_mask)[0]\n",
        "        tmp_preds = torch.argmax(logits,-1).detach().cpu().numpy().tolist()\n",
        "        preds += tmp_preds             \n",
        "    return preds\n",
        "\n",
        "def evaluate(preds,golds,indices_to_label,metric='acc'):\n",
        "    if metric == 'acc':\n",
        "        T,F = 0,0\n",
        "        for pred,gold in zip(preds,golds):\n",
        "            if pred == gold:\n",
        "                T += 1\n",
        "            else:\n",
        "                F += 1\n",
        "        metric = T/(T+F)\n",
        "        print('T:{}, F:{}, F1:{:.2f}'.format(T,F,metric*100))\n",
        "\n",
        "    elif metric == 'f1':\n",
        "        TP,TN,FP,FN = 0,0,0,0\n",
        "        for pred,gold in zip(preds,golds):\n",
        "            if (pred=='1' or pred==1) and (gold=='1' or gold==1):\n",
        "                TP += 1\n",
        "            elif (pred=='1' or pred==1) and (gold!='1' and gold!=1):\n",
        "                FP += 1\n",
        "            elif (pred!='1' and pred!=1) and (gold=='1' or gold==1):\n",
        "                FN += 1\n",
        "            elif (pred!='1' and pred!=1) and (gold!='1' and gold!=1):\n",
        "                TN += 1\n",
        "        if TP==0: \n",
        "            prec,rec,metric = 0,0,0\n",
        "        else:\n",
        "            prec = TP/(TP+FP)\n",
        "            rec = TP/(TP+FN)\n",
        "            metric = 2*prec*rec / (prec+rec)\n",
        "        print('TP:{}, TN:{}, FP:{}, FN:{}, F1:{:.2f}, Precision:{:.2f}, Recall:{:.2f}'.format(TP,TN,FP,FN,metric*100,prec*100,rec*100)) \n",
        "    return metric*100\n",
        "\n",
        "def train(encoded_tokens_train,encoded_tokens_test,encoded_labels_train,encoded_labels_test,label_size,indices_to_label,attention_mask_train,attention_mask_test,i):\n",
        "    config = BertConfig.from_pretrained('emilyalsentzer/Bio_ClinicalBERT',num_labels = label_size)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('emilyalsentzer/Bio_ClinicalBERT',config=config)\n",
        "    #config = BertConfig.from_pretrained('bert-large-uncased',num_labels = label_size)\n",
        "    #model = BertForSequenceClassification.from_pretrained('bert-large-uncased',config=config)\n",
        "    model.cuda()\n",
        "    optimizer = AdamW(model.parameters(),lr=learning_rate)\n",
        "    epochs = max_number_of_epochs\n",
        "    total_steps = len(encoded_tokens_train)*epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)\n",
        "    output_dir = checkpoint_dir\n",
        "    f_best = 0\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      model.train()\n",
        "      data_train = feed_generator(encoded_tokens_train, encoded_labels_train, attention_mask_train, Training=True)\n",
        "      data_test = feed_generator(encoded_tokens_test, encoded_labels_test, attention_mask_test, output=True)\n",
        "            \n",
        "      print(\"Start Epoch {} training...\".format(epoch+1))\n",
        "      start = time.time()\n",
        "      total_loss = 0\n",
        "      batch_loss = 0\n",
        "      for (batch,(token_tensor,label_tensor,attention_mask)) in enumerate(data_train):\n",
        "        model.zero_grad()           \n",
        "        loss = model(token_tensor,token_type_ids=None,attention_mask=attention_mask,labels=label_tensor)[0]\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(),gradient_clipping)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        batch_loss += loss.item()\n",
        "        total_loss += loss.item()\n",
        "        if batch % 100 == 0 and batch != 0:\n",
        "          print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss/100))\n",
        "          batch_loss = 0\n",
        "      print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / batch))\n",
        "      print('Time taken for this epoch {} sec'.format(time.time() - start))\n",
        "      \n",
        "      print('\\nStart evaluation...')\n",
        "                    \n",
        "      preds_test = predict(model,data_test)\n",
        "      f_score_test = evaluate(preds_test,encoded_labels_test,indices_to_label,metric='f1')\n",
        "\n",
        "      if f_score_test > f_best:\n",
        "        patience = 0\n",
        "        f_best = f_score_test\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model\n",
        "        model_to_save.save_pretrained(output_dir +'model '+str(i)+'/')\n",
        "        tokenizer.save_pretrained(output_dir +'model '+str(i)+'/')\n",
        "        print('new highest score!\\n'.format(patience))\n",
        "      else:\n",
        "        if epoch != 0:\n",
        "          patience += 1\n",
        "          print('no improvement for last {} epochs: highest {:.2f}\\n'.format(patience,f_best))\n",
        "        else:\n",
        "          pass\n",
        "      if patience == 3:\n",
        "        print('Early stop!\\n')\n",
        "        break\n",
        "\n",
        "    return(f_best)\n",
        "\n",
        "\n",
        "#Training data shared-task 2018\n",
        "addition_data_path = 'gdrive/My Drive/Colab Notebooks/AnnotationDRUGSInTweets_EMNLPChallenge18_TrainingSetClean.tsv'\n",
        "addition_textprocess = textprocess(addition_data_path, Train=True)\n",
        "addition_df = pd.DataFrame.from_records(addition_textprocess[1:])\n",
        "print('addition_df:',len(addition_df))\n",
        "#Training data shared-task 2020\n",
        "train_data_path = 'gdrive/My Drive/Colab Notebooks/task1_training.tsv'\n",
        "train_textprocess = textprocess(train_data_path, Train=True)\n",
        "train_df = pd.DataFrame.from_records(train_textprocess[1:])\n",
        "print('train_df:',len(train_df))\n",
        "#Validation data shared-task 2020\n",
        "validation_data_path = 'gdrive/My Drive/Colab Notebooks/task1_validation.tsv'\n",
        "validation_textprocess = textprocess(validation_data_path, Train=True)\n",
        "validation_df = pd.DataFrame.from_records(validation_textprocess[1:])\n",
        "print('validation_df:',len(validation_df))\n",
        "#Concatinate training and validation data of shared-task 2020 and training data of shared-task 2018, only keep one sample of duplicates\n",
        "training_df = pd.concat([train_df, validation_df, addition_df])\n",
        "print('training_df:',len(training_df))\n",
        "training_df = training_df.drop_duplicates(keep='first')\n",
        "print('training_df:',len(training_df))\n",
        "training_df.columns = ['tweet','label']\n",
        "print(training_df['label'].value_counts())\n",
        "\n",
        "\"\"\"\n",
        "#Prefilter train set\n",
        "train_addition_mednorm = mednorm(train_addition_df,Train=True) \n",
        "train_addition_mednorm_df = pd.DataFrame.from_records(train_addition_mednorm)\n",
        "print('train_addition_mednorm:',len(train_addition_mednorm_df))\n",
        "train_addition_mednorm_df.columns = ['tweet','label']\n",
        "print(train_addition_mednorm_df['label'].value_counts())\n",
        "train_addition_medword_df=train_addition_df[train_addition_df['tweet'].str.contains(\"drug|med|medication\")]\n",
        "print('train_addition_medword:',len(train_addition_medword_df))\n",
        "train_addition_medword_df.columns = ['tweet','label']\n",
        "print(train_addition_medword_df['label'].value_counts())\n",
        "train_addition_filter_df = pd.concat([train_addition_mednorm_df, train_addition_medword_df])\n",
        "print('train_addition_filter:',len(train_addition_filter_df))\n",
        "train_addition_filter_df = train_addition_filter_df.drop_duplicates(keep='first')\n",
        "print('train_addition_filter:',len(train_addition_filter_df))\n",
        "train_addition_filter_df.columns = ['tweet','label']\n",
        "print(train_addition_filter_df['label'].value_counts())\n",
        "train_addition_lelf_df=pd.concat([train_addition_df, train_addition_filter_df]).drop_duplicates(keep=False)\n",
        "print('train_addition_lelf',len(train_addition_lelf_df))\n",
        "train_addition_lelf_df.columns = ['tweet','label']\n",
        "print(train_addition_lelf_df['label'].value_counts())\n",
        "\n",
        "train_mednorm = mednorm(train_df,Train=True) \n",
        "train_mednorm_df = pd.DataFrame.from_records(train_mednorm)\n",
        "print('train_mednorm:',len(train_mednorm_df))\n",
        "train_mednorm_df.columns = ['tweet','label']\n",
        "print(train_mednorm_df['label'].value_counts())\n",
        "train_medword_df=train_df[train_df['tweet'].str.contains(\"drug|med|medication\")]\n",
        "print('train_medword:',len(train_medword_df))\n",
        "train_medword_df.columns = ['tweet','label']\n",
        "print(train_medword_df['label'].value_counts())\n",
        "train_filter_df = pd.concat([train_mednorm_df, train_medword_df])\n",
        "print('train_filter:',len(train_filter_df))\n",
        "train_filter_df = train_filter_df.drop_duplicates(keep='first')\n",
        "print('train_filter:',len(train_filter_df))\n",
        "train_filter_df.columns = ['tweet','label']\n",
        "print(train_filter_df['label'].value_counts())\n",
        "train_lelf_df=pd.concat([train_df, train_filter_df]).drop_duplicates(keep=False)\n",
        "print('train_lelf',len(train_lelf_df))\n",
        "train_lelf_df.columns = ['tweet','label']\n",
        "print(train_lelf_df['label'].value_counts())\n",
        "\"\"\"\n",
        "\n",
        "# Training 10 models from 10-fold cross-validation\n",
        "X = np.asarray(training_df['tweet'])\n",
        "y = np.asarray(training_df['label'])\n",
        "skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=999)\n",
        "i=1\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "  X_train = X[train_index]\n",
        "  X_test =  X[test_index]\n",
        "  y_train = y[train_index]\n",
        "  y_test = y[test_index]\n",
        "  train_df = pd.DataFrame({'tweet': X_train,'label': y_train})\n",
        "  validate_df = pd.DataFrame({'tweet': X_test,'label': y_test})\n",
        "  encoded_tokens_Train,labels_Train,attention_mask_Train = preprocess(train_df)\n",
        "  encoded_tokens_Test,labels_Test,attention_mask_Test = preprocess(validate_df)\n",
        "  encoded_labels_Train,encoded_labels_Test,indices_to_label,label_size = label_indexing(labels_Train,labels_Test)\n",
        "  train(encoded_tokens_Train,encoded_tokens_Test,encoded_labels_Train,encoded_labels_Test,label_size,indices_to_label,attention_mask_Train,attention_mask_Test,i)\n",
        "  i += 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scispacy in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: nmslib>=1.7.3.6 in /usr/local/lib/python3.6/dist-packages (from scispacy) (2.0.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from scispacy) (0.16.0)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scispacy) (2.3.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from scispacy) (0.22.2.post1)\n",
            "Requirement already satisfied: requests<3.0.0conllu,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scispacy) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scispacy) (1.18.5)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.6/dist-packages (from scispacy) (0.3.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from nmslib>=1.7.3.6->scispacy) (5.4.8)\n",
            "Requirement already satisfied: pybind11>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from nmslib>=1.7.3.6->scispacy) (2.5.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (50.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (7.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.3->scispacy) (1.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.0->scispacy) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.0->scispacy) (3.1.0)\n",
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
            "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
            "Requirement already satisfied (use --upgrade to upgrade): en-core-sci-lg==0.2.4 from https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: spacy>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from en-core-sci-lg==0.2.4) (2.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (7.4.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (50.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-core-sci-lg==0.2.4) (3.1.0)\n",
            "Building wheels for collected packages: en-core-sci-lg\n",
            "  Building wheel for en-core-sci-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-sci-lg: filename=en_core_sci_lg-0.2.4-cp36-none-any.whl size=501343162 sha256=68e7ada9da9e0f761fc7b5f80ddc3ec79e2ac3608c41d24e24623508ca6bb868\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/ab/e5/fa667519032799529ce6a50944a82d6ae3603819cd07836aa2\n",
            "Successfully built en-core-sci-lg\n",
            "Mounted at /content/gdrive\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa293416f61146cdb26b64b4be622406",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=385.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78cc20a9572e43bbbeacac91ee406d10",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "addition_df: 9622\n",
            "train_df: 55419\n",
            "validation_df: 13853\n",
            "training_df: 78894\n",
            "training_df: 72482\n",
            "0    67352\n",
            "1     5130\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:143: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:144: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b13552c533ff4fc283c516957dcae14e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435778770.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start Epoch 1 training...\n",
            "Epoch 1 Batch 100 Loss 0.3411\n",
            "Epoch 1 Batch 200 Loss 0.1425\n",
            "Epoch 1 Batch 300 Loss 0.1163\n",
            "Epoch 1 Batch 400 Loss 0.1407\n",
            "Epoch 1 Batch 500 Loss 0.0971\n",
            "Epoch 1 Batch 600 Loss 0.0695\n",
            "Epoch 1 Batch 700 Loss 0.0851\n",
            "Epoch 1 Batch 800 Loss 0.0857\n",
            "Epoch 1 Batch 900 Loss 0.0598\n",
            "Epoch 1 Batch 1000 Loss 0.0680\n",
            "Epoch 1 Batch 1100 Loss 0.0621\n",
            "Epoch 1 Batch 1200 Loss 0.0707\n",
            "Epoch 1 Batch 1300 Loss 0.0416\n",
            "Epoch 1 Batch 1400 Loss 0.0981\n",
            "Epoch 1 Batch 1500 Loss 0.0545\n",
            "Epoch 1 Batch 1600 Loss 0.0523\n",
            "Epoch 1 Batch 1700 Loss 0.0632\n",
            "Epoch 1 Batch 1800 Loss 0.0635\n",
            "Epoch 1 Batch 1900 Loss 0.0630\n",
            "Epoch 1 Batch 2000 Loss 0.0539\n",
            "Epoch 1 Batch 2100 Loss 0.0554\n",
            "Epoch 1 Batch 2200 Loss 0.0551\n",
            "Epoch 1 Batch 2300 Loss 0.0508\n",
            "Epoch 1 Batch 2400 Loss 0.0690\n",
            "Epoch 1 Batch 2500 Loss 0.0460\n",
            "Epoch 1 Batch 2600 Loss 0.0602\n",
            "Epoch 1 Batch 2700 Loss 0.0458\n",
            "Epoch 1 Batch 2800 Loss 0.0594\n",
            "Epoch 1 Batch 2900 Loss 0.0513\n",
            "Epoch 1 Batch 3000 Loss 0.0528\n",
            "Epoch 1 Batch 3100 Loss 0.0543\n",
            "Epoch 1 Batch 3200 Loss 0.0635\n",
            "Epoch 1 Batch 3300 Loss 0.0539\n",
            "Epoch 1 Batch 3400 Loss 0.0497\n",
            "Epoch 1 Batch 3500 Loss 0.0411\n",
            "Epoch 1 Batch 3600 Loss 0.0396\n",
            "Epoch 1 Batch 3700 Loss 0.0403\n",
            "Epoch 1 Batch 3800 Loss 0.0388\n",
            "Epoch 1 Batch 3900 Loss 0.0304\n",
            "Epoch 1 Batch 4000 Loss 0.0450\n",
            "Epoch 1 Loss 0.0708\n",
            "Time taken for this epoch 258.0774986743927 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:480, TN:6685, FP:51, FN:33, F1:91.95, Precision:90.40, Recall:93.57\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 2 training...\n",
            "Epoch 2 Batch 100 Loss 0.0231\n",
            "Epoch 2 Batch 200 Loss 0.0183\n",
            "Epoch 2 Batch 300 Loss 0.0277\n",
            "Epoch 2 Batch 400 Loss 0.0311\n",
            "Epoch 2 Batch 500 Loss 0.0277\n",
            "Epoch 2 Batch 600 Loss 0.0353\n",
            "Epoch 2 Batch 700 Loss 0.0406\n",
            "Epoch 2 Batch 800 Loss 0.0172\n",
            "Epoch 2 Batch 900 Loss 0.0249\n",
            "Epoch 2 Batch 1000 Loss 0.0428\n",
            "Epoch 2 Batch 1100 Loss 0.0314\n",
            "Epoch 2 Batch 1200 Loss 0.0213\n",
            "Epoch 2 Batch 1300 Loss 0.0187\n",
            "Epoch 2 Batch 1400 Loss 0.0381\n",
            "Epoch 2 Batch 1500 Loss 0.0274\n",
            "Epoch 2 Batch 1600 Loss 0.0292\n",
            "Epoch 2 Batch 1700 Loss 0.0415\n",
            "Epoch 2 Batch 1800 Loss 0.0226\n",
            "Epoch 2 Batch 1900 Loss 0.0419\n",
            "Epoch 2 Batch 2000 Loss 0.0356\n",
            "Epoch 2 Batch 2100 Loss 0.0105\n",
            "Epoch 2 Batch 2200 Loss 0.0300\n",
            "Epoch 2 Batch 2300 Loss 0.0442\n",
            "Epoch 2 Batch 2400 Loss 0.0165\n",
            "Epoch 2 Batch 2500 Loss 0.0442\n",
            "Epoch 2 Batch 2600 Loss 0.0378\n",
            "Epoch 2 Batch 2700 Loss 0.0222\n",
            "Epoch 2 Batch 2800 Loss 0.0301\n",
            "Epoch 2 Batch 2900 Loss 0.0370\n",
            "Epoch 2 Batch 3000 Loss 0.0247\n",
            "Epoch 2 Batch 3100 Loss 0.0086\n",
            "Epoch 2 Batch 3200 Loss 0.0280\n",
            "Epoch 2 Batch 3300 Loss 0.0273\n",
            "Epoch 2 Batch 3400 Loss 0.0298\n",
            "Epoch 2 Batch 3500 Loss 0.0239\n",
            "Epoch 2 Batch 3600 Loss 0.0242\n",
            "Epoch 2 Batch 3700 Loss 0.0315\n",
            "Epoch 2 Batch 3800 Loss 0.0263\n",
            "Epoch 2 Batch 3900 Loss 0.0326\n",
            "Epoch 2 Batch 4000 Loss 0.0371\n",
            "Epoch 2 Loss 0.0290\n",
            "Time taken for this epoch 256.6437511444092 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:466, TN:6709, FP:27, FN:47, F1:92.64, Precision:94.52, Recall:90.84\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 3 training...\n",
            "Epoch 3 Batch 100 Loss 0.0228\n",
            "Epoch 3 Batch 200 Loss 0.0093\n",
            "Epoch 3 Batch 300 Loss 0.0179\n",
            "Epoch 3 Batch 400 Loss 0.0091\n",
            "Epoch 3 Batch 500 Loss 0.0205\n",
            "Epoch 3 Batch 600 Loss 0.0167\n",
            "Epoch 3 Batch 700 Loss 0.0044\n",
            "Epoch 3 Batch 800 Loss 0.0115\n",
            "Epoch 3 Batch 900 Loss 0.0308\n",
            "Epoch 3 Batch 1000 Loss 0.0205\n",
            "Epoch 3 Batch 1100 Loss 0.0117\n",
            "Epoch 3 Batch 1200 Loss 0.0128\n",
            "Epoch 3 Batch 1300 Loss 0.0245\n",
            "Epoch 3 Batch 1400 Loss 0.0126\n",
            "Epoch 3 Batch 1500 Loss 0.0244\n",
            "Epoch 3 Batch 1600 Loss 0.0175\n",
            "Epoch 3 Batch 1700 Loss 0.0263\n",
            "Epoch 3 Batch 1800 Loss 0.0268\n",
            "Epoch 3 Batch 1900 Loss 0.0253\n",
            "Epoch 3 Batch 2000 Loss 0.0268\n",
            "Epoch 3 Batch 2100 Loss 0.0150\n",
            "Epoch 3 Batch 2200 Loss 0.0147\n",
            "Epoch 3 Batch 2300 Loss 0.0186\n",
            "Epoch 3 Batch 2400 Loss 0.0145\n",
            "Epoch 3 Batch 2500 Loss 0.0142\n",
            "Epoch 3 Batch 2600 Loss 0.0114\n",
            "Epoch 3 Batch 2700 Loss 0.0066\n",
            "Epoch 3 Batch 2800 Loss 0.0167\n",
            "Epoch 3 Batch 2900 Loss 0.0206\n",
            "Epoch 3 Batch 3000 Loss 0.0078\n",
            "Epoch 3 Batch 3100 Loss 0.0168\n",
            "Epoch 3 Batch 3200 Loss 0.0105\n",
            "Epoch 3 Batch 3300 Loss 0.0221\n",
            "Epoch 3 Batch 3400 Loss 0.0103\n",
            "Epoch 3 Batch 3500 Loss 0.0042\n",
            "Epoch 3 Batch 3600 Loss 0.0223\n",
            "Epoch 3 Batch 3700 Loss 0.0302\n",
            "Epoch 3 Batch 3800 Loss 0.0164\n",
            "Epoch 3 Batch 3900 Loss 0.0138\n",
            "Epoch 3 Batch 4000 Loss 0.0115\n",
            "Epoch 3 Loss 0.0170\n",
            "Time taken for this epoch 256.3569371700287 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:479, TN:6701, FP:35, FN:34, F1:93.28, Precision:93.19, Recall:93.37\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 4 training...\n",
            "Epoch 4 Batch 100 Loss 0.0194\n",
            "Epoch 4 Batch 200 Loss 0.0065\n",
            "Epoch 4 Batch 300 Loss 0.0072\n",
            "Epoch 4 Batch 400 Loss 0.0048\n",
            "Epoch 4 Batch 500 Loss 0.0048\n",
            "Epoch 4 Batch 600 Loss 0.0034\n",
            "Epoch 4 Batch 700 Loss 0.0072\n",
            "Epoch 4 Batch 800 Loss 0.0123\n",
            "Epoch 4 Batch 900 Loss 0.0144\n",
            "Epoch 4 Batch 1000 Loss 0.0043\n",
            "Epoch 4 Batch 1100 Loss 0.0019\n",
            "Epoch 4 Batch 1200 Loss 0.0077\n",
            "Epoch 4 Batch 1300 Loss 0.0095\n",
            "Epoch 4 Batch 1400 Loss 0.0014\n",
            "Epoch 4 Batch 1500 Loss 0.0139\n",
            "Epoch 4 Batch 1600 Loss 0.0086\n",
            "Epoch 4 Batch 1700 Loss 0.0062\n",
            "Epoch 4 Batch 1800 Loss 0.0088\n",
            "Epoch 4 Batch 1900 Loss 0.0065\n",
            "Epoch 4 Batch 2000 Loss 0.0105\n",
            "Epoch 4 Batch 2100 Loss 0.0049\n",
            "Epoch 4 Batch 2200 Loss 0.0001\n",
            "Epoch 4 Batch 2300 Loss 0.0016\n",
            "Epoch 4 Batch 2400 Loss 0.0196\n",
            "Epoch 4 Batch 2500 Loss 0.0164\n",
            "Epoch 4 Batch 2600 Loss 0.0094\n",
            "Epoch 4 Batch 2700 Loss 0.0183\n",
            "Epoch 4 Batch 2800 Loss 0.0100\n",
            "Epoch 4 Batch 2900 Loss 0.0102\n",
            "Epoch 4 Batch 3000 Loss 0.0102\n",
            "Epoch 4 Batch 3100 Loss 0.0109\n",
            "Epoch 4 Batch 3200 Loss 0.0140\n",
            "Epoch 4 Batch 3300 Loss 0.0029\n",
            "Epoch 4 Batch 3400 Loss 0.0116\n",
            "Epoch 4 Batch 3500 Loss 0.0150\n",
            "Epoch 4 Batch 3600 Loss 0.0174\n",
            "Epoch 4 Batch 3700 Loss 0.0073\n",
            "Epoch 4 Batch 3800 Loss 0.0129\n",
            "Epoch 4 Batch 3900 Loss 0.0039\n",
            "Epoch 4 Batch 4000 Loss 0.0005\n",
            "Epoch 4 Loss 0.0088\n",
            "Time taken for this epoch 257.1993465423584 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:468, TN:6720, FP:16, FN:45, F1:93.88, Precision:96.69, Recall:91.23\n",
            "new highest score!\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start Epoch 1 training...\n",
            "Epoch 1 Batch 100 Loss 0.3136\n",
            "Epoch 1 Batch 200 Loss 0.1549\n",
            "Epoch 1 Batch 300 Loss 0.1289\n",
            "Epoch 1 Batch 400 Loss 0.1256\n",
            "Epoch 1 Batch 500 Loss 0.1015\n",
            "Epoch 1 Batch 600 Loss 0.0811\n",
            "Epoch 1 Batch 700 Loss 0.0815\n",
            "Epoch 1 Batch 800 Loss 0.0768\n",
            "Epoch 1 Batch 900 Loss 0.0751\n",
            "Epoch 1 Batch 1000 Loss 0.1108\n",
            "Epoch 1 Batch 1100 Loss 0.0665\n",
            "Epoch 1 Batch 1200 Loss 0.0762\n",
            "Epoch 1 Batch 1300 Loss 0.0658\n",
            "Epoch 1 Batch 1400 Loss 0.0479\n",
            "Epoch 1 Batch 1500 Loss 0.0524\n",
            "Epoch 1 Batch 1600 Loss 0.0445\n",
            "Epoch 1 Batch 1700 Loss 0.0602\n",
            "Epoch 1 Batch 1800 Loss 0.0752\n",
            "Epoch 1 Batch 1900 Loss 0.0685\n",
            "Epoch 1 Batch 2000 Loss 0.0636\n",
            "Epoch 1 Batch 2100 Loss 0.0415\n",
            "Epoch 1 Batch 2200 Loss 0.0652\n",
            "Epoch 1 Batch 2300 Loss 0.0566\n",
            "Epoch 1 Batch 2400 Loss 0.0637\n",
            "Epoch 1 Batch 2500 Loss 0.0747\n",
            "Epoch 1 Batch 2600 Loss 0.0635\n",
            "Epoch 1 Batch 2700 Loss 0.0385\n",
            "Epoch 1 Batch 2800 Loss 0.0853\n",
            "Epoch 1 Batch 2900 Loss 0.0596\n",
            "Epoch 1 Batch 3000 Loss 0.0568\n",
            "Epoch 1 Batch 3100 Loss 0.0485\n",
            "Epoch 1 Batch 3200 Loss 0.0515\n",
            "Epoch 1 Batch 3300 Loss 0.0366\n",
            "Epoch 1 Batch 3400 Loss 0.0366\n",
            "Epoch 1 Batch 3500 Loss 0.0548\n",
            "Epoch 1 Batch 3600 Loss 0.0605\n",
            "Epoch 1 Batch 3700 Loss 0.0336\n",
            "Epoch 1 Batch 3800 Loss 0.0554\n",
            "Epoch 1 Batch 3900 Loss 0.0499\n",
            "Epoch 1 Batch 4000 Loss 0.0633\n",
            "Epoch 1 Loss 0.0735\n",
            "Time taken for this epoch 256.20714688301086 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:443, TN:6724, FP:12, FN:70, F1:91.53, Precision:97.36, Recall:86.35\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 2 training...\n",
            "Epoch 2 Batch 100 Loss 0.0301\n",
            "Epoch 2 Batch 200 Loss 0.0369\n",
            "Epoch 2 Batch 300 Loss 0.0061\n",
            "Epoch 2 Batch 400 Loss 0.0502\n",
            "Epoch 2 Batch 500 Loss 0.0442\n",
            "Epoch 2 Batch 600 Loss 0.0351\n",
            "Epoch 2 Batch 700 Loss 0.0272\n",
            "Epoch 2 Batch 800 Loss 0.0417\n",
            "Epoch 2 Batch 900 Loss 0.0374\n",
            "Epoch 2 Batch 1000 Loss 0.0225\n",
            "Epoch 2 Batch 1100 Loss 0.0600\n",
            "Epoch 2 Batch 1200 Loss 0.0282\n",
            "Epoch 2 Batch 1300 Loss 0.0308\n",
            "Epoch 2 Batch 1400 Loss 0.0290\n",
            "Epoch 2 Batch 1500 Loss 0.0335\n",
            "Epoch 2 Batch 1600 Loss 0.0312\n",
            "Epoch 2 Batch 1700 Loss 0.0215\n",
            "Epoch 2 Batch 1800 Loss 0.0361\n",
            "Epoch 2 Batch 1900 Loss 0.0252\n",
            "Epoch 2 Batch 2000 Loss 0.0308\n",
            "Epoch 2 Batch 2100 Loss 0.0290\n",
            "Epoch 2 Batch 2200 Loss 0.0310\n",
            "Epoch 2 Batch 2300 Loss 0.0359\n",
            "Epoch 2 Batch 2400 Loss 0.0309\n",
            "Epoch 2 Batch 2500 Loss 0.0360\n",
            "Epoch 2 Batch 2600 Loss 0.0221\n",
            "Epoch 2 Batch 2700 Loss 0.0278\n",
            "Epoch 2 Batch 2800 Loss 0.0183\n",
            "Epoch 2 Batch 2900 Loss 0.0422\n",
            "Epoch 2 Batch 3000 Loss 0.0312\n",
            "Epoch 2 Batch 3100 Loss 0.0349\n",
            "Epoch 2 Batch 3200 Loss 0.0337\n",
            "Epoch 2 Batch 3300 Loss 0.0185\n",
            "Epoch 2 Batch 3400 Loss 0.0311\n",
            "Epoch 2 Batch 3500 Loss 0.0327\n",
            "Epoch 2 Batch 3600 Loss 0.0274\n",
            "Epoch 2 Batch 3700 Loss 0.0528\n",
            "Epoch 2 Batch 3800 Loss 0.0265\n",
            "Epoch 2 Batch 3900 Loss 0.0259\n",
            "Epoch 2 Batch 4000 Loss 0.0387\n",
            "Epoch 2 Loss 0.0319\n",
            "Time taken for this epoch 254.7477159500122 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:473, TN:6704, FP:32, FN:40, F1:92.93, Precision:93.66, Recall:92.20\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 3 training...\n",
            "Epoch 3 Batch 100 Loss 0.0185\n",
            "Epoch 3 Batch 200 Loss 0.0155\n",
            "Epoch 3 Batch 300 Loss 0.0058\n",
            "Epoch 3 Batch 400 Loss 0.0119\n",
            "Epoch 3 Batch 500 Loss 0.0196\n",
            "Epoch 3 Batch 600 Loss 0.0152\n",
            "Epoch 3 Batch 700 Loss 0.0131\n",
            "Epoch 3 Batch 800 Loss 0.0184\n",
            "Epoch 3 Batch 900 Loss 0.0176\n",
            "Epoch 3 Batch 1000 Loss 0.0138\n",
            "Epoch 3 Batch 1100 Loss 0.0261\n",
            "Epoch 3 Batch 1200 Loss 0.0207\n",
            "Epoch 3 Batch 1300 Loss 0.0152\n",
            "Epoch 3 Batch 1400 Loss 0.0175\n",
            "Epoch 3 Batch 1500 Loss 0.0193\n",
            "Epoch 3 Batch 1600 Loss 0.0171\n",
            "Epoch 3 Batch 1700 Loss 0.0179\n",
            "Epoch 3 Batch 1800 Loss 0.0291\n",
            "Epoch 3 Batch 1900 Loss 0.0235\n",
            "Epoch 3 Batch 2000 Loss 0.0122\n",
            "Epoch 3 Batch 2100 Loss 0.0126\n",
            "Epoch 3 Batch 2200 Loss 0.0174\n",
            "Epoch 3 Batch 2300 Loss 0.0303\n",
            "Epoch 3 Batch 2400 Loss 0.0198\n",
            "Epoch 3 Batch 2500 Loss 0.0077\n",
            "Epoch 3 Batch 2600 Loss 0.0351\n",
            "Epoch 3 Batch 2700 Loss 0.0234\n",
            "Epoch 3 Batch 2800 Loss 0.0164\n",
            "Epoch 3 Batch 2900 Loss 0.0114\n",
            "Epoch 3 Batch 3000 Loss 0.0085\n",
            "Epoch 3 Batch 3100 Loss 0.0176\n",
            "Epoch 3 Batch 3200 Loss 0.0191\n",
            "Epoch 3 Batch 3300 Loss 0.0296\n",
            "Epoch 3 Batch 3400 Loss 0.0152\n",
            "Epoch 3 Batch 3500 Loss 0.0259\n",
            "Epoch 3 Batch 3600 Loss 0.0195\n",
            "Epoch 3 Batch 3700 Loss 0.0059\n",
            "Epoch 3 Batch 3800 Loss 0.0164\n",
            "Epoch 3 Batch 3900 Loss 0.0127\n",
            "Epoch 3 Batch 4000 Loss 0.0121\n",
            "Epoch 3 Loss 0.0176\n",
            "Time taken for this epoch 252.99948978424072 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:471, TN:6702, FP:34, FN:42, F1:92.53, Precision:93.27, Recall:91.81\n",
            "no improvement for last 1 epochs: highest 92.93\n",
            "\n",
            "Start Epoch 4 training...\n",
            "Epoch 4 Batch 100 Loss 0.0003\n",
            "Epoch 4 Batch 200 Loss 0.0001\n",
            "Epoch 4 Batch 300 Loss 0.0231\n",
            "Epoch 4 Batch 400 Loss 0.0142\n",
            "Epoch 4 Batch 500 Loss 0.0096\n",
            "Epoch 4 Batch 600 Loss 0.0084\n",
            "Epoch 4 Batch 700 Loss 0.0134\n",
            "Epoch 4 Batch 800 Loss 0.0192\n",
            "Epoch 4 Batch 900 Loss 0.0050\n",
            "Epoch 4 Batch 1000 Loss 0.0004\n",
            "Epoch 4 Batch 1100 Loss 0.0114\n",
            "Epoch 4 Batch 1200 Loss 0.0041\n",
            "Epoch 4 Batch 1300 Loss 0.0132\n",
            "Epoch 4 Batch 1400 Loss 0.0061\n",
            "Epoch 4 Batch 1500 Loss 0.0053\n",
            "Epoch 4 Batch 1600 Loss 0.0023\n",
            "Epoch 4 Batch 1700 Loss 0.0087\n",
            "Epoch 4 Batch 1800 Loss 0.0008\n",
            "Epoch 4 Batch 1900 Loss 0.0061\n",
            "Epoch 4 Batch 2000 Loss 0.0067\n",
            "Epoch 4 Batch 2100 Loss 0.0136\n",
            "Epoch 4 Batch 2200 Loss 0.0118\n",
            "Epoch 4 Batch 2300 Loss 0.0064\n",
            "Epoch 4 Batch 2400 Loss 0.0172\n",
            "Epoch 4 Batch 2500 Loss 0.0013\n",
            "Epoch 4 Batch 2600 Loss 0.0203\n",
            "Epoch 4 Batch 2700 Loss 0.0118\n",
            "Epoch 4 Batch 2800 Loss 0.0047\n",
            "Epoch 4 Batch 2900 Loss 0.0265\n",
            "Epoch 4 Batch 3000 Loss 0.0130\n",
            "Epoch 4 Batch 3100 Loss 0.0093\n",
            "Epoch 4 Batch 3200 Loss 0.0086\n",
            "Epoch 4 Batch 3300 Loss 0.0202\n",
            "Epoch 4 Batch 3400 Loss 0.0175\n",
            "Epoch 4 Batch 3500 Loss 0.0144\n",
            "Epoch 4 Batch 3600 Loss 0.0019\n",
            "Epoch 4 Batch 3700 Loss 0.0253\n",
            "Epoch 4 Batch 3800 Loss 0.0023\n",
            "Epoch 4 Batch 3900 Loss 0.0181\n",
            "Epoch 4 Batch 4000 Loss 0.0004\n",
            "Epoch 4 Loss 0.0100\n",
            "Time taken for this epoch 250.27615690231323 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:473, TN:6714, FP:22, FN:40, F1:93.85, Precision:95.56, Recall:92.20\n",
            "new highest score!\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start Epoch 1 training...\n",
            "Epoch 1 Batch 100 Loss 0.2969\n",
            "Epoch 1 Batch 200 Loss 0.1636\n",
            "Epoch 1 Batch 300 Loss 0.1364\n",
            "Epoch 1 Batch 400 Loss 0.1190\n",
            "Epoch 1 Batch 500 Loss 0.0867\n",
            "Epoch 1 Batch 600 Loss 0.1148\n",
            "Epoch 1 Batch 700 Loss 0.0764\n",
            "Epoch 1 Batch 800 Loss 0.0846\n",
            "Epoch 1 Batch 900 Loss 0.0961\n",
            "Epoch 1 Batch 1000 Loss 0.0768\n",
            "Epoch 1 Batch 1100 Loss 0.0682\n",
            "Epoch 1 Batch 1200 Loss 0.0565\n",
            "Epoch 1 Batch 1300 Loss 0.0638\n",
            "Epoch 1 Batch 1400 Loss 0.0654\n",
            "Epoch 1 Batch 1500 Loss 0.0573\n",
            "Epoch 1 Batch 1600 Loss 0.0495\n",
            "Epoch 1 Batch 1700 Loss 0.0407\n",
            "Epoch 1 Batch 1800 Loss 0.0516\n",
            "Epoch 1 Batch 1900 Loss 0.0582\n",
            "Epoch 1 Batch 2000 Loss 0.0603\n",
            "Epoch 1 Batch 2100 Loss 0.0663\n",
            "Epoch 1 Batch 2200 Loss 0.0455\n",
            "Epoch 1 Batch 2300 Loss 0.0524\n",
            "Epoch 1 Batch 2400 Loss 0.0498\n",
            "Epoch 1 Batch 2500 Loss 0.0401\n",
            "Epoch 1 Batch 2600 Loss 0.0650\n",
            "Epoch 1 Batch 2700 Loss 0.0683\n",
            "Epoch 1 Batch 2800 Loss 0.0440\n",
            "Epoch 1 Batch 2900 Loss 0.0605\n",
            "Epoch 1 Batch 3000 Loss 0.0572\n",
            "Epoch 1 Batch 3100 Loss 0.0417\n",
            "Epoch 1 Batch 3200 Loss 0.0448\n",
            "Epoch 1 Batch 3300 Loss 0.0616\n",
            "Epoch 1 Batch 3400 Loss 0.0615\n",
            "Epoch 1 Batch 3500 Loss 0.0690\n",
            "Epoch 1 Batch 3600 Loss 0.0423\n",
            "Epoch 1 Batch 3700 Loss 0.0339\n",
            "Epoch 1 Batch 3800 Loss 0.0437\n",
            "Epoch 1 Batch 3900 Loss 0.0559\n",
            "Epoch 1 Batch 4000 Loss 0.0577\n",
            "Epoch 1 Loss 0.0715\n",
            "Time taken for this epoch 255.11032247543335 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:487, TN:6683, FP:52, FN:26, F1:92.59, Precision:90.35, Recall:94.93\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 2 training...\n",
            "Epoch 2 Batch 100 Loss 0.0415\n",
            "Epoch 2 Batch 200 Loss 0.0334\n",
            "Epoch 2 Batch 300 Loss 0.0192\n",
            "Epoch 2 Batch 400 Loss 0.0377\n",
            "Epoch 2 Batch 500 Loss 0.0327\n",
            "Epoch 2 Batch 600 Loss 0.0462\n",
            "Epoch 2 Batch 700 Loss 0.0358\n",
            "Epoch 2 Batch 800 Loss 0.0266\n",
            "Epoch 2 Batch 900 Loss 0.0227\n",
            "Epoch 2 Batch 1000 Loss 0.0109\n",
            "Epoch 2 Batch 1100 Loss 0.0451\n",
            "Epoch 2 Batch 1200 Loss 0.0352\n",
            "Epoch 2 Batch 1300 Loss 0.0408\n",
            "Epoch 2 Batch 1400 Loss 0.0403\n",
            "Epoch 2 Batch 1500 Loss 0.0345\n",
            "Epoch 2 Batch 1600 Loss 0.0478\n",
            "Epoch 2 Batch 1700 Loss 0.0355\n",
            "Epoch 2 Batch 1800 Loss 0.0263\n",
            "Epoch 2 Batch 1900 Loss 0.0303\n",
            "Epoch 2 Batch 2000 Loss 0.0302\n",
            "Epoch 2 Batch 2100 Loss 0.0180\n",
            "Epoch 2 Batch 2200 Loss 0.0247\n",
            "Epoch 2 Batch 2300 Loss 0.0261\n",
            "Epoch 2 Batch 2400 Loss 0.0269\n",
            "Epoch 2 Batch 2500 Loss 0.0129\n",
            "Epoch 2 Batch 2600 Loss 0.0229\n",
            "Epoch 2 Batch 2700 Loss 0.0318\n",
            "Epoch 2 Batch 2800 Loss 0.0295\n",
            "Epoch 2 Batch 2900 Loss 0.0341\n",
            "Epoch 2 Batch 3000 Loss 0.0299\n",
            "Epoch 2 Batch 3100 Loss 0.0135\n",
            "Epoch 2 Batch 3200 Loss 0.0419\n",
            "Epoch 2 Batch 3300 Loss 0.0238\n",
            "Epoch 2 Batch 3400 Loss 0.0254\n",
            "Epoch 2 Batch 3500 Loss 0.0208\n",
            "Epoch 2 Batch 3600 Loss 0.0313\n",
            "Epoch 2 Batch 3700 Loss 0.0265\n",
            "Epoch 2 Batch 3800 Loss 0.0377\n",
            "Epoch 2 Batch 3900 Loss 0.0251\n",
            "Epoch 2 Batch 4000 Loss 0.0247\n",
            "Epoch 2 Loss 0.0299\n",
            "Time taken for this epoch 253.34741878509521 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:481, TN:6699, FP:36, FN:32, F1:93.40, Precision:93.04, Recall:93.76\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 3 training...\n",
            "Epoch 3 Batch 100 Loss 0.0133\n",
            "Epoch 3 Batch 200 Loss 0.0139\n",
            "Epoch 3 Batch 300 Loss 0.0133\n",
            "Epoch 3 Batch 400 Loss 0.0263\n",
            "Epoch 3 Batch 500 Loss 0.0062\n",
            "Epoch 3 Batch 600 Loss 0.0114\n",
            "Epoch 3 Batch 700 Loss 0.0174\n",
            "Epoch 3 Batch 800 Loss 0.0113\n",
            "Epoch 3 Batch 900 Loss 0.0116\n",
            "Epoch 3 Batch 1000 Loss 0.0170\n",
            "Epoch 3 Batch 1100 Loss 0.0126\n",
            "Epoch 3 Batch 1200 Loss 0.0196\n",
            "Epoch 3 Batch 1300 Loss 0.0066\n",
            "Epoch 3 Batch 1400 Loss 0.0232\n",
            "Epoch 3 Batch 1500 Loss 0.0170\n",
            "Epoch 3 Batch 1600 Loss 0.0266\n",
            "Epoch 3 Batch 1700 Loss 0.0052\n",
            "Epoch 3 Batch 1800 Loss 0.0276\n",
            "Epoch 3 Batch 1900 Loss 0.0091\n",
            "Epoch 3 Batch 2000 Loss 0.0075\n",
            "Epoch 3 Batch 2100 Loss 0.0206\n",
            "Epoch 3 Batch 2200 Loss 0.0114\n",
            "Epoch 3 Batch 2300 Loss 0.0111\n",
            "Epoch 3 Batch 2400 Loss 0.0240\n",
            "Epoch 3 Batch 2500 Loss 0.0258\n",
            "Epoch 3 Batch 2600 Loss 0.0180\n",
            "Epoch 3 Batch 2700 Loss 0.0167\n",
            "Epoch 3 Batch 2800 Loss 0.0075\n",
            "Epoch 3 Batch 2900 Loss 0.0282\n",
            "Epoch 3 Batch 3000 Loss 0.0285\n",
            "Epoch 3 Batch 3100 Loss 0.0269\n",
            "Epoch 3 Batch 3200 Loss 0.0269\n",
            "Epoch 3 Batch 3300 Loss 0.0248\n",
            "Epoch 3 Batch 3400 Loss 0.0276\n",
            "Epoch 3 Batch 3500 Loss 0.0148\n",
            "Epoch 3 Batch 3600 Loss 0.0177\n",
            "Epoch 3 Batch 3700 Loss 0.0222\n",
            "Epoch 3 Batch 3800 Loss 0.0081\n",
            "Epoch 3 Batch 3900 Loss 0.0170\n",
            "Epoch 3 Batch 4000 Loss 0.0186\n",
            "Epoch 3 Loss 0.0173\n",
            "Time taken for this epoch 250.61977767944336 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:473, TN:6713, FP:22, FN:40, F1:93.85, Precision:95.56, Recall:92.20\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 4 training...\n",
            "Epoch 4 Batch 100 Loss 0.0084\n",
            "Epoch 4 Batch 200 Loss 0.0003\n",
            "Epoch 4 Batch 300 Loss 0.0002\n",
            "Epoch 4 Batch 400 Loss 0.0183\n",
            "Epoch 4 Batch 500 Loss 0.0060\n",
            "Epoch 4 Batch 600 Loss 0.0066\n",
            "Epoch 4 Batch 700 Loss 0.0132\n",
            "Epoch 4 Batch 800 Loss 0.0037\n",
            "Epoch 4 Batch 900 Loss 0.0028\n",
            "Epoch 4 Batch 1000 Loss 0.0223\n",
            "Epoch 4 Batch 1100 Loss 0.0167\n",
            "Epoch 4 Batch 1200 Loss 0.0086\n",
            "Epoch 4 Batch 1300 Loss 0.0079\n",
            "Epoch 4 Batch 1400 Loss 0.0070\n",
            "Epoch 4 Batch 1500 Loss 0.0035\n",
            "Epoch 4 Batch 1600 Loss 0.0015\n",
            "Epoch 4 Batch 1700 Loss 0.0065\n",
            "Epoch 4 Batch 1800 Loss 0.0235\n",
            "Epoch 4 Batch 1900 Loss 0.0081\n",
            "Epoch 4 Batch 2000 Loss 0.0054\n",
            "Epoch 4 Batch 2100 Loss 0.0086\n",
            "Epoch 4 Batch 2200 Loss 0.0080\n",
            "Epoch 4 Batch 2300 Loss 0.0084\n",
            "Epoch 4 Batch 2400 Loss 0.0065\n",
            "Epoch 4 Batch 2500 Loss 0.0154\n",
            "Epoch 4 Batch 2600 Loss 0.0097\n",
            "Epoch 4 Batch 2700 Loss 0.0080\n",
            "Epoch 4 Batch 2800 Loss 0.0053\n",
            "Epoch 4 Batch 2900 Loss 0.0108\n",
            "Epoch 4 Batch 3000 Loss 0.0001\n",
            "Epoch 4 Batch 3100 Loss 0.0016\n",
            "Epoch 4 Batch 3200 Loss 0.0142\n",
            "Epoch 4 Batch 3300 Loss 0.0070\n",
            "Epoch 4 Batch 3400 Loss 0.0068\n",
            "Epoch 4 Batch 3500 Loss 0.0181\n",
            "Epoch 4 Batch 3600 Loss 0.0108\n",
            "Epoch 4 Batch 3700 Loss 0.0068\n",
            "Epoch 4 Batch 3800 Loss 0.0174\n",
            "Epoch 4 Batch 3900 Loss 0.0106\n",
            "Epoch 4 Batch 4000 Loss 0.0002\n",
            "Epoch 4 Loss 0.0085\n",
            "Time taken for this epoch 249.90112900733948 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:484, TN:6696, FP:39, FN:29, F1:93.44, Precision:92.54, Recall:94.35\n",
            "no improvement for last 1 epochs: highest 93.85\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start Epoch 1 training...\n",
            "Epoch 1 Batch 100 Loss 0.3357\n",
            "Epoch 1 Batch 200 Loss 0.1661\n",
            "Epoch 1 Batch 300 Loss 0.1279\n",
            "Epoch 1 Batch 400 Loss 0.1368\n",
            "Epoch 1 Batch 500 Loss 0.0980\n",
            "Epoch 1 Batch 600 Loss 0.1207\n",
            "Epoch 1 Batch 700 Loss 0.0832\n",
            "Epoch 1 Batch 800 Loss 0.1146\n",
            "Epoch 1 Batch 900 Loss 0.0490\n",
            "Epoch 1 Batch 1000 Loss 0.1085\n",
            "Epoch 1 Batch 1100 Loss 0.0476\n",
            "Epoch 1 Batch 1200 Loss 0.0904\n",
            "Epoch 1 Batch 1300 Loss 0.0605\n",
            "Epoch 1 Batch 1400 Loss 0.0546\n",
            "Epoch 1 Batch 1500 Loss 0.0521\n",
            "Epoch 1 Batch 1600 Loss 0.0587\n",
            "Epoch 1 Batch 1700 Loss 0.0709\n",
            "Epoch 1 Batch 1800 Loss 0.0576\n",
            "Epoch 1 Batch 1900 Loss 0.0473\n",
            "Epoch 1 Batch 2000 Loss 0.0583\n",
            "Epoch 1 Batch 2100 Loss 0.0748\n",
            "Epoch 1 Batch 2200 Loss 0.0606\n",
            "Epoch 1 Batch 2300 Loss 0.0553\n",
            "Epoch 1 Batch 2400 Loss 0.0439\n",
            "Epoch 1 Batch 2500 Loss 0.0558\n",
            "Epoch 1 Batch 2600 Loss 0.0555\n",
            "Epoch 1 Batch 2700 Loss 0.0717\n",
            "Epoch 1 Batch 2800 Loss 0.0563\n",
            "Epoch 1 Batch 2900 Loss 0.0561\n",
            "Epoch 1 Batch 3000 Loss 0.0751\n",
            "Epoch 1 Batch 3100 Loss 0.0383\n",
            "Epoch 1 Batch 3200 Loss 0.0493\n",
            "Epoch 1 Batch 3300 Loss 0.0706\n",
            "Epoch 1 Batch 3400 Loss 0.0531\n",
            "Epoch 1 Batch 3500 Loss 0.0433\n",
            "Epoch 1 Batch 3600 Loss 0.0382\n",
            "Epoch 1 Batch 3700 Loss 0.0364\n",
            "Epoch 1 Batch 3800 Loss 0.0498\n",
            "Epoch 1 Batch 3900 Loss 0.0619\n",
            "Epoch 1 Batch 4000 Loss 0.0396\n",
            "Epoch 1 Loss 0.0746\n",
            "Time taken for this epoch 252.78743720054626 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:467, TN:6711, FP:24, FN:46, F1:93.03, Precision:95.11, Recall:91.03\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 2 training...\n",
            "Epoch 2 Batch 100 Loss 0.0359\n",
            "Epoch 2 Batch 200 Loss 0.0371\n",
            "Epoch 2 Batch 300 Loss 0.0428\n",
            "Epoch 2 Batch 400 Loss 0.0317\n",
            "Epoch 2 Batch 500 Loss 0.0317\n",
            "Epoch 2 Batch 600 Loss 0.0307\n",
            "Epoch 2 Batch 700 Loss 0.0290\n",
            "Epoch 2 Batch 800 Loss 0.0306\n",
            "Epoch 2 Batch 900 Loss 0.0236\n",
            "Epoch 2 Batch 1000 Loss 0.0336\n",
            "Epoch 2 Batch 1100 Loss 0.0388\n",
            "Epoch 2 Batch 1200 Loss 0.0331\n",
            "Epoch 2 Batch 1300 Loss 0.0383\n",
            "Epoch 2 Batch 1400 Loss 0.0184\n",
            "Epoch 2 Batch 1500 Loss 0.0506\n",
            "Epoch 2 Batch 1600 Loss 0.0343\n",
            "Epoch 2 Batch 1700 Loss 0.0166\n",
            "Epoch 2 Batch 1800 Loss 0.0347\n",
            "Epoch 2 Batch 1900 Loss 0.0411\n",
            "Epoch 2 Batch 2000 Loss 0.0413\n",
            "Epoch 2 Batch 2100 Loss 0.0395\n",
            "Epoch 2 Batch 2200 Loss 0.0295\n",
            "Epoch 2 Batch 2300 Loss 0.0531\n",
            "Epoch 2 Batch 2400 Loss 0.0301\n",
            "Epoch 2 Batch 2500 Loss 0.0318\n",
            "Epoch 2 Batch 2600 Loss 0.0356\n",
            "Epoch 2 Batch 2700 Loss 0.0257\n",
            "Epoch 2 Batch 2800 Loss 0.0154\n",
            "Epoch 2 Batch 2900 Loss 0.0446\n",
            "Epoch 2 Batch 3000 Loss 0.0275\n",
            "Epoch 2 Batch 3100 Loss 0.0388\n",
            "Epoch 2 Batch 3200 Loss 0.0241\n",
            "Epoch 2 Batch 3300 Loss 0.0292\n",
            "Epoch 2 Batch 3400 Loss 0.0410\n",
            "Epoch 2 Batch 3500 Loss 0.0245\n",
            "Epoch 2 Batch 3600 Loss 0.0414\n",
            "Epoch 2 Batch 3700 Loss 0.0328\n",
            "Epoch 2 Batch 3800 Loss 0.0274\n",
            "Epoch 2 Batch 3900 Loss 0.0276\n",
            "Epoch 2 Batch 4000 Loss 0.0393\n",
            "Epoch 2 Loss 0.0332\n",
            "Time taken for this epoch 250.90809726715088 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:487, TN:6688, FP:47, FN:26, F1:93.03, Precision:91.20, Recall:94.93\n",
            "no improvement for last 1 epochs: highest 93.03\n",
            "\n",
            "Start Epoch 3 training...\n",
            "Epoch 3 Batch 100 Loss 0.0169\n",
            "Epoch 3 Batch 200 Loss 0.0209\n",
            "Epoch 3 Batch 300 Loss 0.0220\n",
            "Epoch 3 Batch 400 Loss 0.0124\n",
            "Epoch 3 Batch 500 Loss 0.0129\n",
            "Epoch 3 Batch 600 Loss 0.0247\n",
            "Epoch 3 Batch 700 Loss 0.0154\n",
            "Epoch 3 Batch 800 Loss 0.0120\n",
            "Epoch 3 Batch 900 Loss 0.0161\n",
            "Epoch 3 Batch 1000 Loss 0.0145\n",
            "Epoch 3 Batch 1100 Loss 0.0214\n",
            "Epoch 3 Batch 1200 Loss 0.0229\n",
            "Epoch 3 Batch 1300 Loss 0.0188\n",
            "Epoch 3 Batch 1400 Loss 0.0149\n",
            "Epoch 3 Batch 1500 Loss 0.0139\n",
            "Epoch 3 Batch 1600 Loss 0.0202\n",
            "Epoch 3 Batch 1700 Loss 0.0240\n",
            "Epoch 3 Batch 1800 Loss 0.0125\n",
            "Epoch 3 Batch 1900 Loss 0.0158\n",
            "Epoch 3 Batch 2000 Loss 0.0233\n",
            "Epoch 3 Batch 2100 Loss 0.0284\n",
            "Epoch 3 Batch 2200 Loss 0.0211\n",
            "Epoch 3 Batch 2300 Loss 0.0336\n",
            "Epoch 3 Batch 2400 Loss 0.0154\n",
            "Epoch 3 Batch 2500 Loss 0.0246\n",
            "Epoch 3 Batch 2600 Loss 0.0164\n",
            "Epoch 3 Batch 2700 Loss 0.0376\n",
            "Epoch 3 Batch 2800 Loss 0.0196\n",
            "Epoch 3 Batch 2900 Loss 0.0183\n",
            "Epoch 3 Batch 3000 Loss 0.0136\n",
            "Epoch 3 Batch 3100 Loss 0.0071\n",
            "Epoch 3 Batch 3200 Loss 0.0183\n",
            "Epoch 3 Batch 3300 Loss 0.0087\n",
            "Epoch 3 Batch 3400 Loss 0.0309\n",
            "Epoch 3 Batch 3500 Loss 0.0223\n",
            "Epoch 3 Batch 3600 Loss 0.0151\n",
            "Epoch 3 Batch 3700 Loss 0.0242\n",
            "Epoch 3 Batch 3800 Loss 0.0157\n",
            "Epoch 3 Batch 3900 Loss 0.0207\n",
            "Epoch 3 Batch 4000 Loss 0.0228\n",
            "Epoch 3 Loss 0.0191\n",
            "Time taken for this epoch 252.58332657814026 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:489, TN:6695, FP:40, FN:24, F1:93.86, Precision:92.44, Recall:95.32\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 4 training...\n",
            "Epoch 4 Batch 100 Loss 0.0203\n",
            "Epoch 4 Batch 200 Loss 0.0134\n",
            "Epoch 4 Batch 300 Loss 0.0086\n",
            "Epoch 4 Batch 400 Loss 0.0002\n",
            "Epoch 4 Batch 500 Loss 0.0170\n",
            "Epoch 4 Batch 600 Loss 0.0094\n",
            "Epoch 4 Batch 700 Loss 0.0138\n",
            "Epoch 4 Batch 800 Loss 0.0001\n",
            "Epoch 4 Batch 900 Loss 0.0139\n",
            "Epoch 4 Batch 1000 Loss 0.0017\n",
            "Epoch 4 Batch 1100 Loss 0.0183\n",
            "Epoch 4 Batch 1200 Loss 0.0032\n",
            "Epoch 4 Batch 1300 Loss 0.0043\n",
            "Epoch 4 Batch 1400 Loss 0.0102\n",
            "Epoch 4 Batch 1500 Loss 0.0089\n",
            "Epoch 4 Batch 1600 Loss 0.0125\n",
            "Epoch 4 Batch 1700 Loss 0.0112\n",
            "Epoch 4 Batch 1800 Loss 0.0097\n",
            "Epoch 4 Batch 1900 Loss 0.0079\n",
            "Epoch 4 Batch 2000 Loss 0.0062\n",
            "Epoch 4 Batch 2100 Loss 0.0004\n",
            "Epoch 4 Batch 2200 Loss 0.0105\n",
            "Epoch 4 Batch 2300 Loss 0.0238\n",
            "Epoch 4 Batch 2400 Loss 0.0124\n",
            "Epoch 4 Batch 2500 Loss 0.0124\n",
            "Epoch 4 Batch 2600 Loss 0.0147\n",
            "Epoch 4 Batch 2700 Loss 0.0147\n",
            "Epoch 4 Batch 2800 Loss 0.0101\n",
            "Epoch 4 Batch 2900 Loss 0.0103\n",
            "Epoch 4 Batch 3000 Loss 0.0087\n",
            "Epoch 4 Batch 3100 Loss 0.0155\n",
            "Epoch 4 Batch 3200 Loss 0.0183\n",
            "Epoch 4 Batch 3300 Loss 0.0074\n",
            "Epoch 4 Batch 3400 Loss 0.0079\n",
            "Epoch 4 Batch 3500 Loss 0.0102\n",
            "Epoch 4 Batch 3600 Loss 0.0083\n",
            "Epoch 4 Batch 3700 Loss 0.0128\n",
            "Epoch 4 Batch 3800 Loss 0.0065\n",
            "Epoch 4 Batch 3900 Loss 0.0154\n",
            "Epoch 4 Batch 4000 Loss 0.0064\n",
            "Epoch 4 Loss 0.0104\n",
            "Time taken for this epoch 253.05653405189514 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:483, TN:6704, FP:31, FN:30, F1:94.06, Precision:93.97, Recall:94.15\n",
            "new highest score!\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start Epoch 1 training...\n",
            "Epoch 1 Batch 100 Loss 0.3453\n",
            "Epoch 1 Batch 200 Loss 0.1385\n",
            "Epoch 1 Batch 300 Loss 0.1539\n",
            "Epoch 1 Batch 400 Loss 0.1207\n",
            "Epoch 1 Batch 500 Loss 0.1140\n",
            "Epoch 1 Batch 600 Loss 0.0830\n",
            "Epoch 1 Batch 700 Loss 0.0933\n",
            "Epoch 1 Batch 800 Loss 0.0713\n",
            "Epoch 1 Batch 900 Loss 0.0571\n",
            "Epoch 1 Batch 1000 Loss 0.0575\n",
            "Epoch 1 Batch 1100 Loss 0.0705\n",
            "Epoch 1 Batch 1200 Loss 0.0823\n",
            "Epoch 1 Batch 1300 Loss 0.0619\n",
            "Epoch 1 Batch 1400 Loss 0.0803\n",
            "Epoch 1 Batch 1500 Loss 0.0636\n",
            "Epoch 1 Batch 1600 Loss 0.0715\n",
            "Epoch 1 Batch 1700 Loss 0.0592\n",
            "Epoch 1 Batch 1800 Loss 0.0475\n",
            "Epoch 1 Batch 1900 Loss 0.0513\n",
            "Epoch 1 Batch 2000 Loss 0.0584\n",
            "Epoch 1 Batch 2100 Loss 0.0827\n",
            "Epoch 1 Batch 2200 Loss 0.0686\n",
            "Epoch 1 Batch 2300 Loss 0.0336\n",
            "Epoch 1 Batch 2400 Loss 0.0465\n",
            "Epoch 1 Batch 2500 Loss 0.0548\n",
            "Epoch 1 Batch 2600 Loss 0.0552\n",
            "Epoch 1 Batch 2700 Loss 0.0517\n",
            "Epoch 1 Batch 2800 Loss 0.0552\n",
            "Epoch 1 Batch 2900 Loss 0.0515\n",
            "Epoch 1 Batch 3000 Loss 0.0288\n",
            "Epoch 1 Batch 3100 Loss 0.0430\n",
            "Epoch 1 Batch 3200 Loss 0.0644\n",
            "Epoch 1 Batch 3300 Loss 0.0512\n",
            "Epoch 1 Batch 3400 Loss 0.0602\n",
            "Epoch 1 Batch 3500 Loss 0.0537\n",
            "Epoch 1 Batch 3600 Loss 0.0481\n",
            "Epoch 1 Batch 3700 Loss 0.0515\n",
            "Epoch 1 Batch 3800 Loss 0.0436\n",
            "Epoch 1 Batch 3900 Loss 0.0556\n",
            "Epoch 1 Batch 4000 Loss 0.0570\n",
            "Epoch 1 Loss 0.0730\n",
            "Time taken for this epoch 254.5955309867859 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:467, TN:6710, FP:25, FN:46, F1:92.94, Precision:94.92, Recall:91.03\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 2 training...\n",
            "Epoch 2 Batch 100 Loss 0.0177\n",
            "Epoch 2 Batch 200 Loss 0.0273\n",
            "Epoch 2 Batch 300 Loss 0.0303\n",
            "Epoch 2 Batch 400 Loss 0.0346\n",
            "Epoch 2 Batch 500 Loss 0.0355\n",
            "Epoch 2 Batch 600 Loss 0.0483\n",
            "Epoch 2 Batch 700 Loss 0.0203\n",
            "Epoch 2 Batch 800 Loss 0.0321\n",
            "Epoch 2 Batch 900 Loss 0.0183\n",
            "Epoch 2 Batch 1000 Loss 0.0343\n",
            "Epoch 2 Batch 1100 Loss 0.0269\n",
            "Epoch 2 Batch 1200 Loss 0.0294\n",
            "Epoch 2 Batch 1300 Loss 0.0251\n",
            "Epoch 2 Batch 1400 Loss 0.0434\n",
            "Epoch 2 Batch 1500 Loss 0.0321\n",
            "Epoch 2 Batch 1600 Loss 0.0267\n",
            "Epoch 2 Batch 1700 Loss 0.0453\n",
            "Epoch 2 Batch 1800 Loss 0.0324\n",
            "Epoch 2 Batch 1900 Loss 0.0266\n",
            "Epoch 2 Batch 2000 Loss 0.0301\n",
            "Epoch 2 Batch 2100 Loss 0.0233\n",
            "Epoch 2 Batch 2200 Loss 0.0303\n",
            "Epoch 2 Batch 2300 Loss 0.0179\n",
            "Epoch 2 Batch 2400 Loss 0.0382\n",
            "Epoch 2 Batch 2500 Loss 0.0174\n",
            "Epoch 2 Batch 2600 Loss 0.0351\n",
            "Epoch 2 Batch 2700 Loss 0.0427\n",
            "Epoch 2 Batch 2800 Loss 0.0064\n",
            "Epoch 2 Batch 2900 Loss 0.0503\n",
            "Epoch 2 Batch 3000 Loss 0.0319\n",
            "Epoch 2 Batch 3100 Loss 0.0396\n",
            "Epoch 2 Batch 3200 Loss 0.0286\n",
            "Epoch 2 Batch 3300 Loss 0.0221\n",
            "Epoch 2 Batch 3400 Loss 0.0364\n",
            "Epoch 2 Batch 3500 Loss 0.0364\n",
            "Epoch 2 Batch 3600 Loss 0.0420\n",
            "Epoch 2 Batch 3700 Loss 0.0367\n",
            "Epoch 2 Batch 3800 Loss 0.0220\n",
            "Epoch 2 Batch 3900 Loss 0.0343\n",
            "Epoch 2 Batch 4000 Loss 0.0181\n",
            "Epoch 2 Loss 0.0303\n",
            "Time taken for this epoch 251.6466362476349 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:484, TN:6706, FP:29, FN:29, F1:94.35, Precision:94.35, Recall:94.35\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 3 training...\n",
            "Epoch 3 Batch 100 Loss 0.0094\n",
            "Epoch 3 Batch 200 Loss 0.0261\n",
            "Epoch 3 Batch 300 Loss 0.0128\n",
            "Epoch 3 Batch 400 Loss 0.0123\n",
            "Epoch 3 Batch 500 Loss 0.0179\n",
            "Epoch 3 Batch 600 Loss 0.0041\n",
            "Epoch 3 Batch 700 Loss 0.0240\n",
            "Epoch 3 Batch 800 Loss 0.0193\n",
            "Epoch 3 Batch 900 Loss 0.0087\n",
            "Epoch 3 Batch 1000 Loss 0.0123\n",
            "Epoch 3 Batch 1100 Loss 0.0254\n",
            "Epoch 3 Batch 1200 Loss 0.0090\n",
            "Epoch 3 Batch 1300 Loss 0.0039\n",
            "Epoch 3 Batch 1400 Loss 0.0163\n",
            "Epoch 3 Batch 1500 Loss 0.0063\n",
            "Epoch 3 Batch 1600 Loss 0.0217\n",
            "Epoch 3 Batch 1700 Loss 0.0180\n",
            "Epoch 3 Batch 1800 Loss 0.0256\n",
            "Epoch 3 Batch 1900 Loss 0.0011\n",
            "Epoch 3 Batch 2000 Loss 0.0215\n",
            "Epoch 3 Batch 2100 Loss 0.0269\n",
            "Epoch 3 Batch 2200 Loss 0.0228\n",
            "Epoch 3 Batch 2300 Loss 0.0174\n",
            "Epoch 3 Batch 2400 Loss 0.0148\n",
            "Epoch 3 Batch 2500 Loss 0.0179\n",
            "Epoch 3 Batch 2600 Loss 0.0095\n",
            "Epoch 3 Batch 2700 Loss 0.0205\n",
            "Epoch 3 Batch 2800 Loss 0.0100\n",
            "Epoch 3 Batch 2900 Loss 0.0169\n",
            "Epoch 3 Batch 3000 Loss 0.0190\n",
            "Epoch 3 Batch 3100 Loss 0.0238\n",
            "Epoch 3 Batch 3200 Loss 0.0173\n",
            "Epoch 3 Batch 3300 Loss 0.0185\n",
            "Epoch 3 Batch 3400 Loss 0.0254\n",
            "Epoch 3 Batch 3500 Loss 0.0356\n",
            "Epoch 3 Batch 3600 Loss 0.0269\n",
            "Epoch 3 Batch 3700 Loss 0.0150\n",
            "Epoch 3 Batch 3800 Loss 0.0106\n",
            "Epoch 3 Batch 3900 Loss 0.0198\n",
            "Epoch 3 Batch 4000 Loss 0.0160\n",
            "Epoch 3 Loss 0.0170\n",
            "Time taken for this epoch 253.57679295539856 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:478, TN:6713, FP:22, FN:35, F1:94.37, Precision:95.60, Recall:93.18\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 4 training...\n",
            "Epoch 4 Batch 100 Loss 0.0087\n",
            "Epoch 4 Batch 200 Loss 0.0099\n",
            "Epoch 4 Batch 300 Loss 0.0045\n",
            "Epoch 4 Batch 400 Loss 0.0071\n",
            "Epoch 4 Batch 500 Loss 0.0042\n",
            "Epoch 4 Batch 600 Loss 0.0170\n",
            "Epoch 4 Batch 700 Loss 0.0038\n",
            "Epoch 4 Batch 800 Loss 0.0087\n",
            "Epoch 4 Batch 900 Loss 0.0121\n",
            "Epoch 4 Batch 1000 Loss 0.0082\n",
            "Epoch 4 Batch 1100 Loss 0.0112\n",
            "Epoch 4 Batch 1200 Loss 0.0093\n",
            "Epoch 4 Batch 1300 Loss 0.0149\n",
            "Epoch 4 Batch 1400 Loss 0.0118\n",
            "Epoch 4 Batch 1500 Loss 0.0157\n",
            "Epoch 4 Batch 1600 Loss 0.0087\n",
            "Epoch 4 Batch 1700 Loss 0.0097\n",
            "Epoch 4 Batch 1800 Loss 0.0164\n",
            "Epoch 4 Batch 1900 Loss 0.0025\n",
            "Epoch 4 Batch 2000 Loss 0.0117\n",
            "Epoch 4 Batch 2100 Loss 0.0026\n",
            "Epoch 4 Batch 2200 Loss 0.0133\n",
            "Epoch 4 Batch 2300 Loss 0.0002\n",
            "Epoch 4 Batch 2400 Loss 0.0040\n",
            "Epoch 4 Batch 2500 Loss 0.0112\n",
            "Epoch 4 Batch 2600 Loss 0.0046\n",
            "Epoch 4 Batch 2700 Loss 0.0080\n",
            "Epoch 4 Batch 2800 Loss 0.0110\n",
            "Epoch 4 Batch 2900 Loss 0.0083\n",
            "Epoch 4 Batch 3000 Loss 0.0098\n",
            "Epoch 4 Batch 3100 Loss 0.0041\n",
            "Epoch 4 Batch 3200 Loss 0.0084\n",
            "Epoch 4 Batch 3300 Loss 0.0123\n",
            "Epoch 4 Batch 3400 Loss 0.0153\n",
            "Epoch 4 Batch 3500 Loss 0.0077\n",
            "Epoch 4 Batch 3600 Loss 0.0122\n",
            "Epoch 4 Batch 3700 Loss 0.0136\n",
            "Epoch 4 Batch 3800 Loss 0.0099\n",
            "Epoch 4 Batch 3900 Loss 0.0062\n",
            "Epoch 4 Batch 4000 Loss 0.0146\n",
            "Epoch 4 Loss 0.0094\n",
            "Time taken for this epoch 251.8547558784485 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:476, TN:6716, FP:19, FN:37, F1:94.44, Precision:96.16, Recall:92.79\n",
            "new highest score!\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start Epoch 1 training...\n",
            "Epoch 1 Batch 100 Loss 0.3164\n",
            "Epoch 1 Batch 200 Loss 0.1743\n",
            "Epoch 1 Batch 300 Loss 0.1275\n",
            "Epoch 1 Batch 400 Loss 0.1113\n",
            "Epoch 1 Batch 500 Loss 0.0963\n",
            "Epoch 1 Batch 600 Loss 0.1165\n",
            "Epoch 1 Batch 700 Loss 0.1014\n",
            "Epoch 1 Batch 800 Loss 0.0756\n",
            "Epoch 1 Batch 900 Loss 0.0716\n",
            "Epoch 1 Batch 1000 Loss 0.0647\n",
            "Epoch 1 Batch 1100 Loss 0.0751\n",
            "Epoch 1 Batch 1200 Loss 0.0612\n",
            "Epoch 1 Batch 1300 Loss 0.0688\n",
            "Epoch 1 Batch 1400 Loss 0.0825\n",
            "Epoch 1 Batch 1500 Loss 0.0547\n",
            "Epoch 1 Batch 1600 Loss 0.0935\n",
            "Epoch 1 Batch 1700 Loss 0.0362\n",
            "Epoch 1 Batch 1800 Loss 0.0766\n",
            "Epoch 1 Batch 1900 Loss 0.0552\n",
            "Epoch 1 Batch 2000 Loss 0.0606\n",
            "Epoch 1 Batch 2100 Loss 0.0418\n",
            "Epoch 1 Batch 2200 Loss 0.0584\n",
            "Epoch 1 Batch 2300 Loss 0.0680\n",
            "Epoch 1 Batch 2400 Loss 0.0658\n",
            "Epoch 1 Batch 2500 Loss 0.0562\n",
            "Epoch 1 Batch 2600 Loss 0.0349\n",
            "Epoch 1 Batch 2700 Loss 0.0500\n",
            "Epoch 1 Batch 2800 Loss 0.0555\n",
            "Epoch 1 Batch 2900 Loss 0.0246\n",
            "Epoch 1 Batch 3000 Loss 0.0524\n",
            "Epoch 1 Batch 3100 Loss 0.0637\n",
            "Epoch 1 Batch 3200 Loss 0.0608\n",
            "Epoch 1 Batch 3300 Loss 0.0355\n",
            "Epoch 1 Batch 3400 Loss 0.0502\n",
            "Epoch 1 Batch 3500 Loss 0.0407\n",
            "Epoch 1 Batch 3600 Loss 0.0567\n",
            "Epoch 1 Batch 3700 Loss 0.0425\n",
            "Epoch 1 Batch 3800 Loss 0.0387\n",
            "Epoch 1 Batch 3900 Loss 0.0208\n",
            "Epoch 1 Batch 4000 Loss 0.0524\n",
            "Epoch 1 Loss 0.0721\n",
            "Time taken for this epoch 255.75585675239563 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:468, TN:6685, FP:50, FN:45, F1:90.79, Precision:90.35, Recall:91.23\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 2 training...\n",
            "Epoch 2 Batch 100 Loss 0.0328\n",
            "Epoch 2 Batch 200 Loss 0.0262\n",
            "Epoch 2 Batch 300 Loss 0.0338\n",
            "Epoch 2 Batch 400 Loss 0.0281\n",
            "Epoch 2 Batch 500 Loss 0.0245\n",
            "Epoch 2 Batch 600 Loss 0.0180\n",
            "Epoch 2 Batch 700 Loss 0.0205\n",
            "Epoch 2 Batch 800 Loss 0.0457\n",
            "Epoch 2 Batch 900 Loss 0.0157\n",
            "Epoch 2 Batch 1000 Loss 0.0177\n",
            "Epoch 2 Batch 1100 Loss 0.0291\n",
            "Epoch 2 Batch 1200 Loss 0.0425\n",
            "Epoch 2 Batch 1300 Loss 0.0210\n",
            "Epoch 2 Batch 1400 Loss 0.0266\n",
            "Epoch 2 Batch 1500 Loss 0.0220\n",
            "Epoch 2 Batch 1600 Loss 0.0274\n",
            "Epoch 2 Batch 1700 Loss 0.0301\n",
            "Epoch 2 Batch 1800 Loss 0.0367\n",
            "Epoch 2 Batch 1900 Loss 0.0262\n",
            "Epoch 2 Batch 2000 Loss 0.0434\n",
            "Epoch 2 Batch 2100 Loss 0.0204\n",
            "Epoch 2 Batch 2200 Loss 0.0340\n",
            "Epoch 2 Batch 2300 Loss 0.0378\n",
            "Epoch 2 Batch 2400 Loss 0.0400\n",
            "Epoch 2 Batch 2500 Loss 0.0248\n",
            "Epoch 2 Batch 2600 Loss 0.0402\n",
            "Epoch 2 Batch 2700 Loss 0.0241\n",
            "Epoch 2 Batch 2800 Loss 0.0313\n",
            "Epoch 2 Batch 2900 Loss 0.0286\n",
            "Epoch 2 Batch 3000 Loss 0.0348\n",
            "Epoch 2 Batch 3100 Loss 0.0306\n",
            "Epoch 2 Batch 3200 Loss 0.0297\n",
            "Epoch 2 Batch 3300 Loss 0.0241\n",
            "Epoch 2 Batch 3400 Loss 0.0416\n",
            "Epoch 2 Batch 3500 Loss 0.0322\n",
            "Epoch 2 Batch 3600 Loss 0.0271\n",
            "Epoch 2 Batch 3700 Loss 0.0418\n",
            "Epoch 2 Batch 3800 Loss 0.0244\n",
            "Epoch 2 Batch 3900 Loss 0.0381\n",
            "Epoch 2 Batch 4000 Loss 0.0509\n",
            "Epoch 2 Loss 0.0305\n",
            "Time taken for this epoch 255.32846689224243 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:484, TN:6683, FP:52, FN:29, F1:92.28, Precision:90.30, Recall:94.35\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 3 training...\n",
            "Epoch 3 Batch 100 Loss 0.0197\n",
            "Epoch 3 Batch 200 Loss 0.0144\n",
            "Epoch 3 Batch 300 Loss 0.0174\n",
            "Epoch 3 Batch 400 Loss 0.0165\n",
            "Epoch 3 Batch 500 Loss 0.0207\n",
            "Epoch 3 Batch 600 Loss 0.0088\n",
            "Epoch 3 Batch 700 Loss 0.0116\n",
            "Epoch 3 Batch 800 Loss 0.0201\n",
            "Epoch 3 Batch 900 Loss 0.0238\n",
            "Epoch 3 Batch 1000 Loss 0.0174\n",
            "Epoch 3 Batch 1100 Loss 0.0241\n",
            "Epoch 3 Batch 1200 Loss 0.0096\n",
            "Epoch 3 Batch 1300 Loss 0.0186\n",
            "Epoch 3 Batch 1400 Loss 0.0131\n",
            "Epoch 3 Batch 1500 Loss 0.0090\n",
            "Epoch 3 Batch 1600 Loss 0.0160\n",
            "Epoch 3 Batch 1700 Loss 0.0337\n",
            "Epoch 3 Batch 1800 Loss 0.0036\n",
            "Epoch 3 Batch 1900 Loss 0.0099\n",
            "Epoch 3 Batch 2000 Loss 0.0147\n",
            "Epoch 3 Batch 2100 Loss 0.0107\n",
            "Epoch 3 Batch 2200 Loss 0.0105\n",
            "Epoch 3 Batch 2300 Loss 0.0246\n",
            "Epoch 3 Batch 2400 Loss 0.0235\n",
            "Epoch 3 Batch 2500 Loss 0.0265\n",
            "Epoch 3 Batch 2600 Loss 0.0318\n",
            "Epoch 3 Batch 2700 Loss 0.0161\n",
            "Epoch 3 Batch 2800 Loss 0.0140\n",
            "Epoch 3 Batch 2900 Loss 0.0145\n",
            "Epoch 3 Batch 3000 Loss 0.0179\n",
            "Epoch 3 Batch 3100 Loss 0.0070\n",
            "Epoch 3 Batch 3200 Loss 0.0112\n",
            "Epoch 3 Batch 3300 Loss 0.0203\n",
            "Epoch 3 Batch 3400 Loss 0.0171\n",
            "Epoch 3 Batch 3500 Loss 0.0133\n",
            "Epoch 3 Batch 3600 Loss 0.0104\n",
            "Epoch 3 Batch 3700 Loss 0.0134\n",
            "Epoch 3 Batch 3800 Loss 0.0132\n",
            "Epoch 3 Batch 3900 Loss 0.0099\n",
            "Epoch 3 Batch 4000 Loss 0.0174\n",
            "Epoch 3 Loss 0.0161\n",
            "Time taken for this epoch 257.1061177253723 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:482, TN:6690, FP:45, FN:31, F1:92.69, Precision:91.46, Recall:93.96\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 4 training...\n",
            "Epoch 4 Batch 100 Loss 0.0020\n",
            "Epoch 4 Batch 200 Loss 0.0006\n",
            "Epoch 4 Batch 300 Loss 0.0028\n",
            "Epoch 4 Batch 400 Loss 0.0094\n",
            "Epoch 4 Batch 500 Loss 0.0006\n",
            "Epoch 4 Batch 600 Loss 0.0120\n",
            "Epoch 4 Batch 700 Loss 0.0094\n",
            "Epoch 4 Batch 800 Loss 0.0073\n",
            "Epoch 4 Batch 900 Loss 0.0077\n",
            "Epoch 4 Batch 1000 Loss 0.0048\n",
            "Epoch 4 Batch 1100 Loss 0.0018\n",
            "Epoch 4 Batch 1200 Loss 0.0060\n",
            "Epoch 4 Batch 1300 Loss 0.0045\n",
            "Epoch 4 Batch 1400 Loss 0.0051\n",
            "Epoch 4 Batch 1500 Loss 0.0108\n",
            "Epoch 4 Batch 1600 Loss 0.0011\n",
            "Epoch 4 Batch 1700 Loss 0.0118\n",
            "Epoch 4 Batch 1800 Loss 0.0154\n",
            "Epoch 4 Batch 1900 Loss 0.0126\n",
            "Epoch 4 Batch 2000 Loss 0.0048\n",
            "Epoch 4 Batch 2100 Loss 0.0049\n",
            "Epoch 4 Batch 2200 Loss 0.0200\n",
            "Epoch 4 Batch 2300 Loss 0.0094\n",
            "Epoch 4 Batch 2400 Loss 0.0053\n",
            "Epoch 4 Batch 2500 Loss 0.0164\n",
            "Epoch 4 Batch 2600 Loss 0.0054\n",
            "Epoch 4 Batch 2700 Loss 0.0049\n",
            "Epoch 4 Batch 2800 Loss 0.0247\n",
            "Epoch 4 Batch 2900 Loss 0.0176\n",
            "Epoch 4 Batch 3000 Loss 0.0059\n",
            "Epoch 4 Batch 3100 Loss 0.0079\n",
            "Epoch 4 Batch 3200 Loss 0.0178\n",
            "Epoch 4 Batch 3300 Loss 0.0132\n",
            "Epoch 4 Batch 3400 Loss 0.0021\n",
            "Epoch 4 Batch 3500 Loss 0.0135\n",
            "Epoch 4 Batch 3600 Loss 0.0020\n",
            "Epoch 4 Batch 3700 Loss 0.0111\n",
            "Epoch 4 Batch 3800 Loss 0.0232\n",
            "Epoch 4 Batch 3900 Loss 0.0153\n",
            "Epoch 4 Batch 4000 Loss 0.0110\n",
            "Epoch 4 Loss 0.0090\n",
            "Time taken for this epoch 256.77218413352966 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:472, TN:6705, FP:30, FN:41, F1:93.00, Precision:94.02, Recall:92.01\n",
            "new highest score!\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start Epoch 1 training...\n",
            "Epoch 1 Batch 100 Loss 0.2899\n",
            "Epoch 1 Batch 200 Loss 0.1520\n",
            "Epoch 1 Batch 300 Loss 0.1298\n",
            "Epoch 1 Batch 400 Loss 0.1449\n",
            "Epoch 1 Batch 500 Loss 0.1132\n",
            "Epoch 1 Batch 600 Loss 0.0827\n",
            "Epoch 1 Batch 700 Loss 0.0907\n",
            "Epoch 1 Batch 800 Loss 0.1093\n",
            "Epoch 1 Batch 900 Loss 0.0745\n",
            "Epoch 1 Batch 1000 Loss 0.0649\n",
            "Epoch 1 Batch 1100 Loss 0.0833\n",
            "Epoch 1 Batch 1200 Loss 0.0555\n",
            "Epoch 1 Batch 1300 Loss 0.0691\n",
            "Epoch 1 Batch 1400 Loss 0.0613\n",
            "Epoch 1 Batch 1500 Loss 0.0547\n",
            "Epoch 1 Batch 1600 Loss 0.0649\n",
            "Epoch 1 Batch 1700 Loss 0.0554\n",
            "Epoch 1 Batch 1800 Loss 0.0516\n",
            "Epoch 1 Batch 1900 Loss 0.0460\n",
            "Epoch 1 Batch 2000 Loss 0.0559\n",
            "Epoch 1 Batch 2100 Loss 0.0649\n",
            "Epoch 1 Batch 2200 Loss 0.0306\n",
            "Epoch 1 Batch 2300 Loss 0.0741\n",
            "Epoch 1 Batch 2400 Loss 0.0625\n",
            "Epoch 1 Batch 2500 Loss 0.0636\n",
            "Epoch 1 Batch 2600 Loss 0.0674\n",
            "Epoch 1 Batch 2700 Loss 0.0599\n",
            "Epoch 1 Batch 2800 Loss 0.0447\n",
            "Epoch 1 Batch 2900 Loss 0.0461\n",
            "Epoch 1 Batch 3000 Loss 0.0410\n",
            "Epoch 1 Batch 3100 Loss 0.0635\n",
            "Epoch 1 Batch 3200 Loss 0.0359\n",
            "Epoch 1 Batch 3300 Loss 0.0306\n",
            "Epoch 1 Batch 3400 Loss 0.0443\n",
            "Epoch 1 Batch 3500 Loss 0.0522\n",
            "Epoch 1 Batch 3600 Loss 0.0551\n",
            "Epoch 1 Batch 3700 Loss 0.0338\n",
            "Epoch 1 Batch 3800 Loss 0.0380\n",
            "Epoch 1 Batch 3900 Loss 0.0484\n",
            "Epoch 1 Batch 4000 Loss 0.0326\n",
            "Epoch 1 Loss 0.0706\n",
            "Time taken for this epoch 259.6218090057373 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:456, TN:6708, FP:27, FN:57, F1:91.57, Precision:94.41, Recall:88.89\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 2 training...\n",
            "Epoch 2 Batch 100 Loss 0.0263\n",
            "Epoch 2 Batch 200 Loss 0.0288\n",
            "Epoch 2 Batch 300 Loss 0.0100\n",
            "Epoch 2 Batch 400 Loss 0.0440\n",
            "Epoch 2 Batch 500 Loss 0.0261\n",
            "Epoch 2 Batch 600 Loss 0.0347\n",
            "Epoch 2 Batch 700 Loss 0.0193\n",
            "Epoch 2 Batch 800 Loss 0.0227\n",
            "Epoch 2 Batch 900 Loss 0.0263\n",
            "Epoch 2 Batch 1000 Loss 0.0254\n",
            "Epoch 2 Batch 1100 Loss 0.0367\n",
            "Epoch 2 Batch 1200 Loss 0.0300\n",
            "Epoch 2 Batch 1300 Loss 0.0401\n",
            "Epoch 2 Batch 1400 Loss 0.0385\n",
            "Epoch 2 Batch 1500 Loss 0.0122\n",
            "Epoch 2 Batch 1600 Loss 0.0372\n",
            "Epoch 2 Batch 1700 Loss 0.0129\n",
            "Epoch 2 Batch 1800 Loss 0.0307\n",
            "Epoch 2 Batch 1900 Loss 0.0293\n",
            "Epoch 2 Batch 2000 Loss 0.0343\n",
            "Epoch 2 Batch 2100 Loss 0.0598\n",
            "Epoch 2 Batch 2200 Loss 0.0248\n",
            "Epoch 2 Batch 2300 Loss 0.0266\n",
            "Epoch 2 Batch 2400 Loss 0.0492\n",
            "Epoch 2 Batch 2500 Loss 0.0251\n",
            "Epoch 2 Batch 2600 Loss 0.0313\n",
            "Epoch 2 Batch 2700 Loss 0.0217\n",
            "Epoch 2 Batch 2800 Loss 0.0309\n",
            "Epoch 2 Batch 2900 Loss 0.0429\n",
            "Epoch 2 Batch 3000 Loss 0.0396\n",
            "Epoch 2 Batch 3100 Loss 0.0271\n",
            "Epoch 2 Batch 3200 Loss 0.0194\n",
            "Epoch 2 Batch 3300 Loss 0.0253\n",
            "Epoch 2 Batch 3400 Loss 0.0358\n",
            "Epoch 2 Batch 3500 Loss 0.0530\n",
            "Epoch 2 Batch 3600 Loss 0.0331\n",
            "Epoch 2 Batch 3700 Loss 0.0340\n",
            "Epoch 2 Batch 3800 Loss 0.0463\n",
            "Epoch 2 Batch 3900 Loss 0.0287\n",
            "Epoch 2 Batch 4000 Loss 0.0274\n",
            "Epoch 2 Loss 0.0314\n",
            "Time taken for this epoch 258.10050868988037 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:469, TN:6698, FP:37, FN:44, F1:92.05, Precision:92.69, Recall:91.42\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 3 training...\n",
            "Epoch 3 Batch 100 Loss 0.0156\n",
            "Epoch 3 Batch 200 Loss 0.0047\n",
            "Epoch 3 Batch 300 Loss 0.0138\n",
            "Epoch 3 Batch 400 Loss 0.0196\n",
            "Epoch 3 Batch 500 Loss 0.0126\n",
            "Epoch 3 Batch 600 Loss 0.0130\n",
            "Epoch 3 Batch 700 Loss 0.0091\n",
            "Epoch 3 Batch 800 Loss 0.0104\n",
            "Epoch 3 Batch 900 Loss 0.0113\n",
            "Epoch 3 Batch 1000 Loss 0.0316\n",
            "Epoch 3 Batch 1100 Loss 0.0119\n",
            "Epoch 3 Batch 1200 Loss 0.0184\n",
            "Epoch 3 Batch 1300 Loss 0.0208\n",
            "Epoch 3 Batch 1400 Loss 0.0072\n",
            "Epoch 3 Batch 1500 Loss 0.0036\n",
            "Epoch 3 Batch 1600 Loss 0.0156\n",
            "Epoch 3 Batch 1700 Loss 0.0170\n",
            "Epoch 3 Batch 1800 Loss 0.0372\n",
            "Epoch 3 Batch 1900 Loss 0.0288\n",
            "Epoch 3 Batch 2000 Loss 0.0090\n",
            "Epoch 3 Batch 2100 Loss 0.0132\n",
            "Epoch 3 Batch 2200 Loss 0.0240\n",
            "Epoch 3 Batch 2300 Loss 0.0148\n",
            "Epoch 3 Batch 2400 Loss 0.0258\n",
            "Epoch 3 Batch 2500 Loss 0.0128\n",
            "Epoch 3 Batch 2600 Loss 0.0123\n",
            "Epoch 3 Batch 2700 Loss 0.0067\n",
            "Epoch 3 Batch 2800 Loss 0.0039\n",
            "Epoch 3 Batch 2900 Loss 0.0142\n",
            "Epoch 3 Batch 3000 Loss 0.0091\n",
            "Epoch 3 Batch 3100 Loss 0.0231\n",
            "Epoch 3 Batch 3200 Loss 0.0154\n",
            "Epoch 3 Batch 3300 Loss 0.0075\n",
            "Epoch 3 Batch 3400 Loss 0.0144\n",
            "Epoch 3 Batch 3500 Loss 0.0176\n",
            "Epoch 3 Batch 3600 Loss 0.0069\n",
            "Epoch 3 Batch 3700 Loss 0.0177\n",
            "Epoch 3 Batch 3800 Loss 0.0397\n",
            "Epoch 3 Batch 3900 Loss 0.0198\n",
            "Epoch 3 Batch 4000 Loss 0.0043\n",
            "Epoch 3 Loss 0.0153\n",
            "Time taken for this epoch 257.4940230846405 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:481, TN:6693, FP:42, FN:32, F1:92.86, Precision:91.97, Recall:93.76\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 4 training...\n",
            "Epoch 4 Batch 100 Loss 0.0007\n",
            "Epoch 4 Batch 200 Loss 0.0010\n",
            "Epoch 4 Batch 300 Loss 0.0122\n",
            "Epoch 4 Batch 400 Loss 0.0145\n",
            "Epoch 4 Batch 500 Loss 0.0127\n",
            "Epoch 4 Batch 600 Loss 0.0187\n",
            "Epoch 4 Batch 700 Loss 0.0052\n",
            "Epoch 4 Batch 800 Loss 0.0140\n",
            "Epoch 4 Batch 900 Loss 0.0065\n",
            "Epoch 4 Batch 1000 Loss 0.0132\n",
            "Epoch 4 Batch 1100 Loss 0.0024\n",
            "Epoch 4 Batch 1200 Loss 0.0058\n",
            "Epoch 4 Batch 1300 Loss 0.0071\n",
            "Epoch 4 Batch 1400 Loss 0.0084\n",
            "Epoch 4 Batch 1500 Loss 0.0086\n",
            "Epoch 4 Batch 1600 Loss 0.0094\n",
            "Epoch 4 Batch 1700 Loss 0.0096\n",
            "Epoch 4 Batch 1800 Loss 0.0110\n",
            "Epoch 4 Batch 1900 Loss 0.0070\n",
            "Epoch 4 Batch 2000 Loss 0.0036\n",
            "Epoch 4 Batch 2100 Loss 0.0048\n",
            "Epoch 4 Batch 2200 Loss 0.0138\n",
            "Epoch 4 Batch 2300 Loss 0.0179\n",
            "Epoch 4 Batch 2400 Loss 0.0065\n",
            "Epoch 4 Batch 2500 Loss 0.0001\n",
            "Epoch 4 Batch 2600 Loss 0.0050\n",
            "Epoch 4 Batch 2700 Loss 0.0108\n",
            "Epoch 4 Batch 2800 Loss 0.0031\n",
            "Epoch 4 Batch 2900 Loss 0.0039\n",
            "Epoch 4 Batch 3000 Loss 0.0238\n",
            "Epoch 4 Batch 3100 Loss 0.0074\n",
            "Epoch 4 Batch 3200 Loss 0.0232\n",
            "Epoch 4 Batch 3300 Loss 0.0161\n",
            "Epoch 4 Batch 3400 Loss 0.0109\n",
            "Epoch 4 Batch 3500 Loss 0.0167\n",
            "Epoch 4 Batch 3600 Loss 0.0076\n",
            "Epoch 4 Batch 3700 Loss 0.0117\n",
            "Epoch 4 Batch 3800 Loss 0.0037\n",
            "Epoch 4 Batch 3900 Loss 0.0066\n",
            "Epoch 4 Batch 4000 Loss 0.0239\n",
            "Epoch 4 Loss 0.0097\n",
            "Time taken for this epoch 256.8855767250061 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:473, TN:6705, FP:30, FN:40, F1:93.11, Precision:94.04, Recall:92.20\n",
            "new highest score!\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start Epoch 1 training...\n",
            "Epoch 1 Batch 100 Loss 0.2617\n",
            "Epoch 1 Batch 200 Loss 0.1795\n",
            "Epoch 1 Batch 300 Loss 0.1140\n",
            "Epoch 1 Batch 400 Loss 0.1265\n",
            "Epoch 1 Batch 500 Loss 0.1174\n",
            "Epoch 1 Batch 600 Loss 0.0833\n",
            "Epoch 1 Batch 700 Loss 0.1043\n",
            "Epoch 1 Batch 800 Loss 0.0744\n",
            "Epoch 1 Batch 900 Loss 0.0573\n",
            "Epoch 1 Batch 1000 Loss 0.0816\n",
            "Epoch 1 Batch 1100 Loss 0.0814\n",
            "Epoch 1 Batch 1200 Loss 0.0837\n",
            "Epoch 1 Batch 1300 Loss 0.0841\n",
            "Epoch 1 Batch 1400 Loss 0.0691\n",
            "Epoch 1 Batch 1500 Loss 0.0499\n",
            "Epoch 1 Batch 1600 Loss 0.0740\n",
            "Epoch 1 Batch 1700 Loss 0.0388\n",
            "Epoch 1 Batch 1800 Loss 0.0614\n",
            "Epoch 1 Batch 1900 Loss 0.0524\n",
            "Epoch 1 Batch 2000 Loss 0.0614\n",
            "Epoch 1 Batch 2100 Loss 0.0682\n",
            "Epoch 1 Batch 2200 Loss 0.0523\n",
            "Epoch 1 Batch 2300 Loss 0.0643\n",
            "Epoch 1 Batch 2400 Loss 0.0532\n",
            "Epoch 1 Batch 2500 Loss 0.0656\n",
            "Epoch 1 Batch 2600 Loss 0.0488\n",
            "Epoch 1 Batch 2700 Loss 0.0458\n",
            "Epoch 1 Batch 2800 Loss 0.0677\n",
            "Epoch 1 Batch 2900 Loss 0.0506\n",
            "Epoch 1 Batch 3000 Loss 0.0465\n",
            "Epoch 1 Batch 3100 Loss 0.0362\n",
            "Epoch 1 Batch 3200 Loss 0.0486\n",
            "Epoch 1 Batch 3300 Loss 0.0592\n",
            "Epoch 1 Batch 3400 Loss 0.0469\n",
            "Epoch 1 Batch 3500 Loss 0.0562\n",
            "Epoch 1 Batch 3600 Loss 0.0336\n",
            "Epoch 1 Batch 3700 Loss 0.0327\n",
            "Epoch 1 Batch 3800 Loss 0.0561\n",
            "Epoch 1 Batch 3900 Loss 0.0510\n",
            "Epoch 1 Batch 4000 Loss 0.0600\n",
            "Epoch 1 Loss 0.0715\n",
            "Time taken for this epoch 258.7253768444061 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:444, TN:6696, FP:39, FN:69, F1:89.16, Precision:91.93, Recall:86.55\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 2 training...\n",
            "Epoch 2 Batch 100 Loss 0.0253\n",
            "Epoch 2 Batch 200 Loss 0.0241\n",
            "Epoch 2 Batch 300 Loss 0.0280\n",
            "Epoch 2 Batch 400 Loss 0.0148\n",
            "Epoch 2 Batch 500 Loss 0.0385\n",
            "Epoch 2 Batch 600 Loss 0.0197\n",
            "Epoch 2 Batch 700 Loss 0.0203\n",
            "Epoch 2 Batch 800 Loss 0.0391\n",
            "Epoch 2 Batch 900 Loss 0.0473\n",
            "Epoch 2 Batch 1000 Loss 0.0347\n",
            "Epoch 2 Batch 1100 Loss 0.0350\n",
            "Epoch 2 Batch 1200 Loss 0.0228\n",
            "Epoch 2 Batch 1300 Loss 0.0179\n",
            "Epoch 2 Batch 1400 Loss 0.0308\n",
            "Epoch 2 Batch 1500 Loss 0.0313\n",
            "Epoch 2 Batch 1600 Loss 0.0278\n",
            "Epoch 2 Batch 1700 Loss 0.0245\n",
            "Epoch 2 Batch 1800 Loss 0.0423\n",
            "Epoch 2 Batch 1900 Loss 0.0415\n",
            "Epoch 2 Batch 2000 Loss 0.0234\n",
            "Epoch 2 Batch 2100 Loss 0.0424\n",
            "Epoch 2 Batch 2200 Loss 0.0382\n",
            "Epoch 2 Batch 2300 Loss 0.0415\n",
            "Epoch 2 Batch 2400 Loss 0.0432\n",
            "Epoch 2 Batch 2500 Loss 0.0219\n",
            "Epoch 2 Batch 2600 Loss 0.0399\n",
            "Epoch 2 Batch 2700 Loss 0.0280\n",
            "Epoch 2 Batch 2800 Loss 0.0297\n",
            "Epoch 2 Batch 2900 Loss 0.0259\n",
            "Epoch 2 Batch 3000 Loss 0.0340\n",
            "Epoch 2 Batch 3100 Loss 0.0303\n",
            "Epoch 2 Batch 3200 Loss 0.0312\n",
            "Epoch 2 Batch 3300 Loss 0.0372\n",
            "Epoch 2 Batch 3400 Loss 0.0287\n",
            "Epoch 2 Batch 3500 Loss 0.0374\n",
            "Epoch 2 Batch 3600 Loss 0.0505\n",
            "Epoch 2 Batch 3700 Loss 0.0367\n",
            "Epoch 2 Batch 3800 Loss 0.0357\n",
            "Epoch 2 Batch 3900 Loss 0.0271\n",
            "Epoch 2 Batch 4000 Loss 0.0218\n",
            "Epoch 2 Loss 0.0316\n",
            "Time taken for this epoch 257.19165873527527 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:469, TN:6696, FP:39, FN:44, F1:91.87, Precision:92.32, Recall:91.42\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 3 training...\n",
            "Epoch 3 Batch 100 Loss 0.0150\n",
            "Epoch 3 Batch 200 Loss 0.0194\n",
            "Epoch 3 Batch 300 Loss 0.0105\n",
            "Epoch 3 Batch 400 Loss 0.0156\n",
            "Epoch 3 Batch 500 Loss 0.0130\n",
            "Epoch 3 Batch 600 Loss 0.0166\n",
            "Epoch 3 Batch 700 Loss 0.0047\n",
            "Epoch 3 Batch 800 Loss 0.0003\n",
            "Epoch 3 Batch 900 Loss 0.0170\n",
            "Epoch 3 Batch 1000 Loss 0.0204\n",
            "Epoch 3 Batch 1100 Loss 0.0094\n",
            "Epoch 3 Batch 1200 Loss 0.0075\n",
            "Epoch 3 Batch 1300 Loss 0.0124\n",
            "Epoch 3 Batch 1400 Loss 0.0202\n",
            "Epoch 3 Batch 1500 Loss 0.0156\n",
            "Epoch 3 Batch 1600 Loss 0.0137\n",
            "Epoch 3 Batch 1700 Loss 0.0223\n",
            "Epoch 3 Batch 1800 Loss 0.0184\n",
            "Epoch 3 Batch 1900 Loss 0.0145\n",
            "Epoch 3 Batch 2000 Loss 0.0141\n",
            "Epoch 3 Batch 2100 Loss 0.0238\n",
            "Epoch 3 Batch 2200 Loss 0.0205\n",
            "Epoch 3 Batch 2300 Loss 0.0204\n",
            "Epoch 3 Batch 2400 Loss 0.0158\n",
            "Epoch 3 Batch 2500 Loss 0.0109\n",
            "Epoch 3 Batch 2600 Loss 0.0153\n",
            "Epoch 3 Batch 2700 Loss 0.0250\n",
            "Epoch 3 Batch 2800 Loss 0.0249\n",
            "Epoch 3 Batch 2900 Loss 0.0187\n",
            "Epoch 3 Batch 3000 Loss 0.0111\n",
            "Epoch 3 Batch 3100 Loss 0.0257\n",
            "Epoch 3 Batch 3200 Loss 0.0176\n",
            "Epoch 3 Batch 3300 Loss 0.0194\n",
            "Epoch 3 Batch 3400 Loss 0.0161\n",
            "Epoch 3 Batch 3500 Loss 0.0149\n",
            "Epoch 3 Batch 3600 Loss 0.0168\n",
            "Epoch 3 Batch 3700 Loss 0.0163\n",
            "Epoch 3 Batch 3800 Loss 0.0180\n",
            "Epoch 3 Batch 3900 Loss 0.0237\n",
            "Epoch 3 Batch 4000 Loss 0.0124\n",
            "Epoch 3 Loss 0.0160\n",
            "Time taken for this epoch 256.8523976802826 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:467, TN:6701, FP:34, FN:46, F1:92.11, Precision:93.21, Recall:91.03\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 4 training...\n",
            "Epoch 4 Batch 100 Loss 0.0065\n",
            "Epoch 4 Batch 200 Loss 0.0020\n",
            "Epoch 4 Batch 300 Loss 0.0134\n",
            "Epoch 4 Batch 400 Loss 0.0007\n",
            "Epoch 4 Batch 500 Loss 0.0086\n",
            "Epoch 4 Batch 600 Loss 0.0060\n",
            "Epoch 4 Batch 700 Loss 0.0018\n",
            "Epoch 4 Batch 800 Loss 0.0122\n",
            "Epoch 4 Batch 900 Loss 0.0163\n",
            "Epoch 4 Batch 1000 Loss 0.0010\n",
            "Epoch 4 Batch 1100 Loss 0.0085\n",
            "Epoch 4 Batch 1200 Loss 0.0119\n",
            "Epoch 4 Batch 1300 Loss 0.0021\n",
            "Epoch 4 Batch 1400 Loss 0.0106\n",
            "Epoch 4 Batch 1500 Loss 0.0075\n",
            "Epoch 4 Batch 1600 Loss 0.0002\n",
            "Epoch 4 Batch 1700 Loss 0.0095\n",
            "Epoch 4 Batch 1800 Loss 0.0107\n",
            "Epoch 4 Batch 1900 Loss 0.0090\n",
            "Epoch 4 Batch 2000 Loss 0.0239\n",
            "Epoch 4 Batch 2100 Loss 0.0082\n",
            "Epoch 4 Batch 2200 Loss 0.0197\n",
            "Epoch 4 Batch 2300 Loss 0.0121\n",
            "Epoch 4 Batch 2400 Loss 0.0026\n",
            "Epoch 4 Batch 2500 Loss 0.0044\n",
            "Epoch 4 Batch 2600 Loss 0.0176\n",
            "Epoch 4 Batch 2700 Loss 0.0170\n",
            "Epoch 4 Batch 2800 Loss 0.0251\n",
            "Epoch 4 Batch 2900 Loss 0.0057\n",
            "Epoch 4 Batch 3000 Loss 0.0057\n",
            "Epoch 4 Batch 3100 Loss 0.0181\n",
            "Epoch 4 Batch 3200 Loss 0.0106\n",
            "Epoch 4 Batch 3300 Loss 0.0100\n",
            "Epoch 4 Batch 3400 Loss 0.0180\n",
            "Epoch 4 Batch 3500 Loss 0.0044\n",
            "Epoch 4 Batch 3600 Loss 0.0062\n",
            "Epoch 4 Batch 3700 Loss 0.0069\n",
            "Epoch 4 Batch 3800 Loss 0.0195\n",
            "Epoch 4 Batch 3900 Loss 0.0044\n",
            "Epoch 4 Batch 4000 Loss 0.0079\n",
            "Epoch 4 Loss 0.0095\n",
            "Time taken for this epoch 256.51909732818604 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:478, TN:6680, FP:55, FN:35, F1:91.40, Precision:89.68, Recall:93.18\n",
            "no improvement for last 1 epochs: highest 92.11\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start Epoch 1 training...\n",
            "Epoch 1 Batch 100 Loss 0.3194\n",
            "Epoch 1 Batch 200 Loss 0.1736\n",
            "Epoch 1 Batch 300 Loss 0.1219\n",
            "Epoch 1 Batch 400 Loss 0.1094\n",
            "Epoch 1 Batch 500 Loss 0.0988\n",
            "Epoch 1 Batch 600 Loss 0.1263\n",
            "Epoch 1 Batch 700 Loss 0.0885\n",
            "Epoch 1 Batch 800 Loss 0.0808\n",
            "Epoch 1 Batch 900 Loss 0.0736\n",
            "Epoch 1 Batch 1000 Loss 0.0840\n",
            "Epoch 1 Batch 1100 Loss 0.0732\n",
            "Epoch 1 Batch 1200 Loss 0.0679\n",
            "Epoch 1 Batch 1300 Loss 0.0806\n",
            "Epoch 1 Batch 1400 Loss 0.0582\n",
            "Epoch 1 Batch 1500 Loss 0.0673\n",
            "Epoch 1 Batch 1600 Loss 0.0600\n",
            "Epoch 1 Batch 1700 Loss 0.0646\n",
            "Epoch 1 Batch 1800 Loss 0.0558\n",
            "Epoch 1 Batch 1900 Loss 0.0750\n",
            "Epoch 1 Batch 2000 Loss 0.0547\n",
            "Epoch 1 Batch 2100 Loss 0.0443\n",
            "Epoch 1 Batch 2200 Loss 0.0668\n",
            "Epoch 1 Batch 2300 Loss 0.0582\n",
            "Epoch 1 Batch 2400 Loss 0.0619\n",
            "Epoch 1 Batch 2500 Loss 0.0660\n",
            "Epoch 1 Batch 2600 Loss 0.0491\n",
            "Epoch 1 Batch 2700 Loss 0.0673\n",
            "Epoch 1 Batch 2800 Loss 0.0442\n",
            "Epoch 1 Batch 2900 Loss 0.0579\n",
            "Epoch 1 Batch 3000 Loss 0.0340\n",
            "Epoch 1 Batch 3100 Loss 0.0595\n",
            "Epoch 1 Batch 3200 Loss 0.0467\n",
            "Epoch 1 Batch 3300 Loss 0.0434\n",
            "Epoch 1 Batch 3400 Loss 0.0460\n",
            "Epoch 1 Batch 3500 Loss 0.0417\n",
            "Epoch 1 Batch 3600 Loss 0.0564\n",
            "Epoch 1 Batch 3700 Loss 0.0311\n",
            "Epoch 1 Batch 3800 Loss 0.0322\n",
            "Epoch 1 Batch 3900 Loss 0.0508\n",
            "Epoch 1 Batch 4000 Loss 0.0537\n",
            "Epoch 1 Loss 0.0736\n",
            "Time taken for this epoch 259.45067620277405 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:475, TN:6691, FP:44, FN:38, F1:92.05, Precision:91.52, Recall:92.59\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 2 training...\n",
            "Epoch 2 Batch 100 Loss 0.0351\n",
            "Epoch 2 Batch 200 Loss 0.0155\n",
            "Epoch 2 Batch 300 Loss 0.0443\n",
            "Epoch 2 Batch 400 Loss 0.0266\n",
            "Epoch 2 Batch 500 Loss 0.0266\n",
            "Epoch 2 Batch 600 Loss 0.0415\n",
            "Epoch 2 Batch 700 Loss 0.0355\n",
            "Epoch 2 Batch 800 Loss 0.0374\n",
            "Epoch 2 Batch 900 Loss 0.0296\n",
            "Epoch 2 Batch 1000 Loss 0.0550\n",
            "Epoch 2 Batch 1100 Loss 0.0226\n",
            "Epoch 2 Batch 1200 Loss 0.0350\n",
            "Epoch 2 Batch 1300 Loss 0.0292\n",
            "Epoch 2 Batch 1400 Loss 0.0330\n",
            "Epoch 2 Batch 1500 Loss 0.0356\n",
            "Epoch 2 Batch 1600 Loss 0.0296\n",
            "Epoch 2 Batch 1700 Loss 0.0505\n",
            "Epoch 2 Batch 1800 Loss 0.0274\n",
            "Epoch 2 Batch 1900 Loss 0.0136\n",
            "Epoch 2 Batch 2000 Loss 0.0309\n",
            "Epoch 2 Batch 2100 Loss 0.0339\n",
            "Epoch 2 Batch 2200 Loss 0.0449\n",
            "Epoch 2 Batch 2300 Loss 0.0232\n",
            "Epoch 2 Batch 2400 Loss 0.0293\n",
            "Epoch 2 Batch 2500 Loss 0.0310\n",
            "Epoch 2 Batch 2600 Loss 0.0298\n",
            "Epoch 2 Batch 2700 Loss 0.0400\n",
            "Epoch 2 Batch 2800 Loss 0.0412\n",
            "Epoch 2 Batch 2900 Loss 0.0432\n",
            "Epoch 2 Batch 3000 Loss 0.0399\n",
            "Epoch 2 Batch 3100 Loss 0.0227\n",
            "Epoch 2 Batch 3200 Loss 0.0358\n",
            "Epoch 2 Batch 3300 Loss 0.0357\n",
            "Epoch 2 Batch 3400 Loss 0.0351\n",
            "Epoch 2 Batch 3500 Loss 0.0262\n",
            "Epoch 2 Batch 3600 Loss 0.0302\n",
            "Epoch 2 Batch 3700 Loss 0.0192\n",
            "Epoch 2 Batch 3800 Loss 0.0320\n",
            "Epoch 2 Batch 3900 Loss 0.0273\n",
            "Epoch 2 Batch 4000 Loss 0.0168\n",
            "Epoch 2 Loss 0.0321\n",
            "Time taken for this epoch 258.35355591773987 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:482, TN:6713, FP:22, FN:31, F1:94.79, Precision:95.63, Recall:93.96\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 3 training...\n",
            "Epoch 3 Batch 100 Loss 0.0150\n",
            "Epoch 3 Batch 200 Loss 0.0128\n",
            "Epoch 3 Batch 300 Loss 0.0154\n",
            "Epoch 3 Batch 400 Loss 0.0146\n",
            "Epoch 3 Batch 500 Loss 0.0092\n",
            "Epoch 3 Batch 600 Loss 0.0080\n",
            "Epoch 3 Batch 700 Loss 0.0160\n",
            "Epoch 3 Batch 800 Loss 0.0405\n",
            "Epoch 3 Batch 900 Loss 0.0174\n",
            "Epoch 3 Batch 1000 Loss 0.0363\n",
            "Epoch 3 Batch 1100 Loss 0.0151\n",
            "Epoch 3 Batch 1200 Loss 0.0104\n",
            "Epoch 3 Batch 1300 Loss 0.0056\n",
            "Epoch 3 Batch 1400 Loss 0.0042\n",
            "Epoch 3 Batch 1500 Loss 0.0101\n",
            "Epoch 3 Batch 1600 Loss 0.0319\n",
            "Epoch 3 Batch 1700 Loss 0.0132\n",
            "Epoch 3 Batch 1800 Loss 0.0264\n",
            "Epoch 3 Batch 1900 Loss 0.0225\n",
            "Epoch 3 Batch 2000 Loss 0.0221\n",
            "Epoch 3 Batch 2100 Loss 0.0230\n",
            "Epoch 3 Batch 2200 Loss 0.0084\n",
            "Epoch 3 Batch 2300 Loss 0.0292\n",
            "Epoch 3 Batch 2400 Loss 0.0123\n",
            "Epoch 3 Batch 2500 Loss 0.0150\n",
            "Epoch 3 Batch 2600 Loss 0.0230\n",
            "Epoch 3 Batch 2700 Loss 0.0263\n",
            "Epoch 3 Batch 2800 Loss 0.0141\n",
            "Epoch 3 Batch 2900 Loss 0.0243\n",
            "Epoch 3 Batch 3000 Loss 0.0255\n",
            "Epoch 3 Batch 3100 Loss 0.0195\n",
            "Epoch 3 Batch 3200 Loss 0.0187\n",
            "Epoch 3 Batch 3300 Loss 0.0039\n",
            "Epoch 3 Batch 3400 Loss 0.0145\n",
            "Epoch 3 Batch 3500 Loss 0.0166\n",
            "Epoch 3 Batch 3600 Loss 0.0290\n",
            "Epoch 3 Batch 3700 Loss 0.0214\n",
            "Epoch 3 Batch 3800 Loss 0.0258\n",
            "Epoch 3 Batch 3900 Loss 0.0101\n",
            "Epoch 3 Batch 4000 Loss 0.0172\n",
            "Epoch 3 Loss 0.0182\n",
            "Time taken for this epoch 258.1412432193756 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:490, TN:6684, FP:51, FN:23, F1:92.98, Precision:90.57, Recall:95.52\n",
            "no improvement for last 1 epochs: highest 94.79\n",
            "\n",
            "Start Epoch 4 training...\n",
            "Epoch 4 Batch 100 Loss 0.0038\n",
            "Epoch 4 Batch 200 Loss 0.0038\n",
            "Epoch 4 Batch 300 Loss 0.0011\n",
            "Epoch 4 Batch 400 Loss 0.0130\n",
            "Epoch 4 Batch 500 Loss 0.0061\n",
            "Epoch 4 Batch 600 Loss 0.0044\n",
            "Epoch 4 Batch 700 Loss 0.0090\n",
            "Epoch 4 Batch 800 Loss 0.0009\n",
            "Epoch 4 Batch 900 Loss 0.0109\n",
            "Epoch 4 Batch 1000 Loss 0.0102\n",
            "Epoch 4 Batch 1100 Loss 0.0136\n",
            "Epoch 4 Batch 1200 Loss 0.0073\n",
            "Epoch 4 Batch 1300 Loss 0.0189\n",
            "Epoch 4 Batch 1400 Loss 0.0041\n",
            "Epoch 4 Batch 1500 Loss 0.0118\n",
            "Epoch 4 Batch 1600 Loss 0.0094\n",
            "Epoch 4 Batch 1700 Loss 0.0034\n",
            "Epoch 4 Batch 1800 Loss 0.0065\n",
            "Epoch 4 Batch 1900 Loss 0.0101\n",
            "Epoch 4 Batch 2000 Loss 0.0096\n",
            "Epoch 4 Batch 2100 Loss 0.0024\n",
            "Epoch 4 Batch 2200 Loss 0.0219\n",
            "Epoch 4 Batch 2300 Loss 0.0068\n",
            "Epoch 4 Batch 2400 Loss 0.0160\n",
            "Epoch 4 Batch 2500 Loss 0.0133\n",
            "Epoch 4 Batch 2600 Loss 0.0100\n",
            "Epoch 4 Batch 2700 Loss 0.0001\n",
            "Epoch 4 Batch 2800 Loss 0.0091\n",
            "Epoch 4 Batch 2900 Loss 0.0116\n",
            "Epoch 4 Batch 3000 Loss 0.0075\n",
            "Epoch 4 Batch 3100 Loss 0.0049\n",
            "Epoch 4 Batch 3200 Loss 0.0220\n",
            "Epoch 4 Batch 3300 Loss 0.0131\n",
            "Epoch 4 Batch 3400 Loss 0.0179\n",
            "Epoch 4 Batch 3500 Loss 0.0184\n",
            "Epoch 4 Batch 3600 Loss 0.0025\n",
            "Epoch 4 Batch 3700 Loss 0.0195\n",
            "Epoch 4 Batch 3800 Loss 0.0053\n",
            "Epoch 4 Batch 3900 Loss 0.0112\n",
            "Epoch 4 Batch 4000 Loss 0.0081\n",
            "Epoch 4 Loss 0.0094\n",
            "Time taken for this epoch 256.38801884651184 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:483, TN:6708, FP:27, FN:30, F1:94.43, Precision:94.71, Recall:94.15\n",
            "no improvement for last 2 epochs: highest 94.79\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start Epoch 1 training...\n",
            "Epoch 1 Batch 100 Loss 0.3038\n",
            "Epoch 1 Batch 200 Loss 0.1832\n",
            "Epoch 1 Batch 300 Loss 0.1269\n",
            "Epoch 1 Batch 400 Loss 0.1020\n",
            "Epoch 1 Batch 500 Loss 0.0965\n",
            "Epoch 1 Batch 600 Loss 0.1096\n",
            "Epoch 1 Batch 700 Loss 0.0802\n",
            "Epoch 1 Batch 800 Loss 0.1206\n",
            "Epoch 1 Batch 900 Loss 0.0890\n",
            "Epoch 1 Batch 1000 Loss 0.0811\n",
            "Epoch 1 Batch 1100 Loss 0.0480\n",
            "Epoch 1 Batch 1200 Loss 0.0803\n",
            "Epoch 1 Batch 1300 Loss 0.0619\n",
            "Epoch 1 Batch 1400 Loss 0.0704\n",
            "Epoch 1 Batch 1500 Loss 0.0634\n",
            "Epoch 1 Batch 1600 Loss 0.0518\n",
            "Epoch 1 Batch 1700 Loss 0.0633\n",
            "Epoch 1 Batch 1800 Loss 0.0730\n",
            "Epoch 1 Batch 1900 Loss 0.0538\n",
            "Epoch 1 Batch 2000 Loss 0.0618\n",
            "Epoch 1 Batch 2100 Loss 0.0582\n",
            "Epoch 1 Batch 2200 Loss 0.0483\n",
            "Epoch 1 Batch 2300 Loss 0.0787\n",
            "Epoch 1 Batch 2400 Loss 0.0537\n",
            "Epoch 1 Batch 2500 Loss 0.0691\n",
            "Epoch 1 Batch 2600 Loss 0.0537\n",
            "Epoch 1 Batch 2700 Loss 0.0499\n",
            "Epoch 1 Batch 2800 Loss 0.0308\n",
            "Epoch 1 Batch 2900 Loss 0.0650\n",
            "Epoch 1 Batch 3000 Loss 0.0636\n",
            "Epoch 1 Batch 3100 Loss 0.0384\n",
            "Epoch 1 Batch 3200 Loss 0.0379\n",
            "Epoch 1 Batch 3300 Loss 0.0448\n",
            "Epoch 1 Batch 3400 Loss 0.0394\n",
            "Epoch 1 Batch 3500 Loss 0.0357\n",
            "Epoch 1 Batch 3600 Loss 0.0569\n",
            "Epoch 1 Batch 3700 Loss 0.0573\n",
            "Epoch 1 Batch 3800 Loss 0.0564\n",
            "Epoch 1 Batch 3900 Loss 0.0439\n",
            "Epoch 1 Batch 4000 Loss 0.0368\n",
            "Epoch 1 Loss 0.0729\n",
            "Time taken for this epoch 259.8975553512573 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:471, TN:6705, FP:30, FN:42, F1:92.90, Precision:94.01, Recall:91.81\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 2 training...\n",
            "Epoch 2 Batch 100 Loss 0.0267\n",
            "Epoch 2 Batch 200 Loss 0.0174\n",
            "Epoch 2 Batch 300 Loss 0.0254\n",
            "Epoch 2 Batch 400 Loss 0.0389\n",
            "Epoch 2 Batch 500 Loss 0.0390\n",
            "Epoch 2 Batch 600 Loss 0.0271\n",
            "Epoch 2 Batch 700 Loss 0.0339\n",
            "Epoch 2 Batch 800 Loss 0.0285\n",
            "Epoch 2 Batch 900 Loss 0.0532\n",
            "Epoch 2 Batch 1000 Loss 0.0245\n",
            "Epoch 2 Batch 1100 Loss 0.0254\n",
            "Epoch 2 Batch 1200 Loss 0.0365\n",
            "Epoch 2 Batch 1300 Loss 0.0224\n",
            "Epoch 2 Batch 1400 Loss 0.0229\n",
            "Epoch 2 Batch 1500 Loss 0.0406\n",
            "Epoch 2 Batch 1600 Loss 0.0407\n",
            "Epoch 2 Batch 1700 Loss 0.0425\n",
            "Epoch 2 Batch 1800 Loss 0.0338\n",
            "Epoch 2 Batch 1900 Loss 0.0252\n",
            "Epoch 2 Batch 2000 Loss 0.0311\n",
            "Epoch 2 Batch 2100 Loss 0.0556\n",
            "Epoch 2 Batch 2200 Loss 0.0354\n",
            "Epoch 2 Batch 2300 Loss 0.0312\n",
            "Epoch 2 Batch 2400 Loss 0.0265\n",
            "Epoch 2 Batch 2500 Loss 0.0285\n",
            "Epoch 2 Batch 2600 Loss 0.0293\n",
            "Epoch 2 Batch 2700 Loss 0.0340\n",
            "Epoch 2 Batch 2800 Loss 0.0244\n",
            "Epoch 2 Batch 2900 Loss 0.0271\n",
            "Epoch 2 Batch 3000 Loss 0.0283\n",
            "Epoch 2 Batch 3100 Loss 0.0153\n",
            "Epoch 2 Batch 3200 Loss 0.0333\n",
            "Epoch 2 Batch 3300 Loss 0.0210\n",
            "Epoch 2 Batch 3400 Loss 0.0311\n",
            "Epoch 2 Batch 3500 Loss 0.0395\n",
            "Epoch 2 Batch 3600 Loss 0.0255\n",
            "Epoch 2 Batch 3700 Loss 0.0302\n",
            "Epoch 2 Batch 3800 Loss 0.0320\n",
            "Epoch 2 Batch 3900 Loss 0.0274\n",
            "Epoch 2 Batch 4000 Loss 0.0376\n",
            "Epoch 2 Loss 0.0313\n",
            "Time taken for this epoch 257.8161060810089 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:465, TN:6716, FP:19, FN:48, F1:93.28, Precision:96.07, Recall:90.64\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 3 training...\n",
            "Epoch 3 Batch 100 Loss 0.0186\n",
            "Epoch 3 Batch 200 Loss 0.0126\n",
            "Epoch 3 Batch 300 Loss 0.0158\n",
            "Epoch 3 Batch 400 Loss 0.0110\n",
            "Epoch 3 Batch 500 Loss 0.0157\n",
            "Epoch 3 Batch 600 Loss 0.0060\n",
            "Epoch 3 Batch 700 Loss 0.0295\n",
            "Epoch 3 Batch 800 Loss 0.0105\n",
            "Epoch 3 Batch 900 Loss 0.0268\n",
            "Epoch 3 Batch 1000 Loss 0.0159\n",
            "Epoch 3 Batch 1100 Loss 0.0110\n",
            "Epoch 3 Batch 1200 Loss 0.0063\n",
            "Epoch 3 Batch 1300 Loss 0.0233\n",
            "Epoch 3 Batch 1400 Loss 0.0154\n",
            "Epoch 3 Batch 1500 Loss 0.0324\n",
            "Epoch 3 Batch 1600 Loss 0.0199\n",
            "Epoch 3 Batch 1700 Loss 0.0155\n",
            "Epoch 3 Batch 1800 Loss 0.0109\n",
            "Epoch 3 Batch 1900 Loss 0.0236\n",
            "Epoch 3 Batch 2000 Loss 0.0237\n",
            "Epoch 3 Batch 2100 Loss 0.0088\n",
            "Epoch 3 Batch 2200 Loss 0.0147\n",
            "Epoch 3 Batch 2300 Loss 0.0155\n",
            "Epoch 3 Batch 2400 Loss 0.0146\n",
            "Epoch 3 Batch 2500 Loss 0.0267\n",
            "Epoch 3 Batch 2600 Loss 0.0055\n",
            "Epoch 3 Batch 2700 Loss 0.0175\n",
            "Epoch 3 Batch 2800 Loss 0.0284\n",
            "Epoch 3 Batch 2900 Loss 0.0120\n",
            "Epoch 3 Batch 3000 Loss 0.0164\n",
            "Epoch 3 Batch 3100 Loss 0.0194\n",
            "Epoch 3 Batch 3200 Loss 0.0140\n",
            "Epoch 3 Batch 3300 Loss 0.0229\n",
            "Epoch 3 Batch 3400 Loss 0.0215\n",
            "Epoch 3 Batch 3500 Loss 0.0151\n",
            "Epoch 3 Batch 3600 Loss 0.0077\n",
            "Epoch 3 Batch 3700 Loss 0.0259\n",
            "Epoch 3 Batch 3800 Loss 0.0158\n",
            "Epoch 3 Batch 3900 Loss 0.0135\n",
            "Epoch 3 Batch 4000 Loss 0.0130\n",
            "Epoch 3 Loss 0.0167\n",
            "Time taken for this epoch 257.3958640098572 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:467, TN:6716, FP:19, FN:46, F1:93.49, Precision:96.09, Recall:91.03\n",
            "new highest score!\n",
            "\n",
            "Start Epoch 4 training...\n",
            "Epoch 4 Batch 100 Loss 0.0106\n",
            "Epoch 4 Batch 200 Loss 0.0045\n",
            "Epoch 4 Batch 300 Loss 0.0098\n",
            "Epoch 4 Batch 400 Loss 0.0062\n",
            "Epoch 4 Batch 500 Loss 0.0145\n",
            "Epoch 4 Batch 600 Loss 0.0086\n",
            "Epoch 4 Batch 700 Loss 0.0134\n",
            "Epoch 4 Batch 800 Loss 0.0111\n",
            "Epoch 4 Batch 900 Loss 0.0019\n",
            "Epoch 4 Batch 1000 Loss 0.0046\n",
            "Epoch 4 Batch 1100 Loss 0.0045\n",
            "Epoch 4 Batch 1200 Loss 0.0035\n",
            "Epoch 4 Batch 1300 Loss 0.0163\n",
            "Epoch 4 Batch 1400 Loss 0.0136\n",
            "Epoch 4 Batch 1500 Loss 0.0153\n",
            "Epoch 4 Batch 1600 Loss 0.0060\n",
            "Epoch 4 Batch 1700 Loss 0.0100\n",
            "Epoch 4 Batch 1800 Loss 0.0045\n",
            "Epoch 4 Batch 1900 Loss 0.0070\n",
            "Epoch 4 Batch 2000 Loss 0.0142\n",
            "Epoch 4 Batch 2100 Loss 0.0027\n",
            "Epoch 4 Batch 2200 Loss 0.0104\n",
            "Epoch 4 Batch 2300 Loss 0.0027\n",
            "Epoch 4 Batch 2400 Loss 0.0086\n",
            "Epoch 4 Batch 2500 Loss 0.0256\n",
            "Epoch 4 Batch 2600 Loss 0.0117\n",
            "Epoch 4 Batch 2700 Loss 0.0079\n",
            "Epoch 4 Batch 2800 Loss 0.0164\n",
            "Epoch 4 Batch 2900 Loss 0.0147\n",
            "Epoch 4 Batch 3000 Loss 0.0189\n",
            "Epoch 4 Batch 3100 Loss 0.0060\n",
            "Epoch 4 Batch 3200 Loss 0.0062\n",
            "Epoch 4 Batch 3300 Loss 0.0082\n",
            "Epoch 4 Batch 3400 Loss 0.0176\n",
            "Epoch 4 Batch 3500 Loss 0.0057\n",
            "Epoch 4 Batch 3600 Loss 0.0091\n",
            "Epoch 4 Batch 3700 Loss 0.0196\n",
            "Epoch 4 Batch 3800 Loss 0.0008\n",
            "Epoch 4 Batch 3900 Loss 0.0230\n",
            "Epoch 4 Batch 4000 Loss 0.0218\n",
            "Epoch 4 Loss 0.0103\n",
            "Time taken for this epoch 256.5134699344635 sec\n",
            "\n",
            "Start evaluation...\n",
            "TP:480, TN:6693, FP:42, FN:33, F1:92.75, Precision:91.95, Recall:93.57\n",
            "no improvement for last 1 epochs: highest 93.49\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIpYiXHDDYqM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "outputId": "2e1eedd4-408b-4ea1-c216-6d582e0aa8dc"
      },
      "source": [
        "#Test dataset shared-task 2020\n",
        "test_data_path = 'test_data'\n",
        "test_textprocess = textprocess(test_data_path,Train=False)\n",
        "testing_df = pd.DataFrame.from_records(test_textprocess[1:])\n",
        "testing_df.columns = ['tweet_id','tweet','label']\n",
        "print('test_df:',len(test_df))\n",
        "print(test_df['label'].value_counts())\n",
        "#Pre-filter test dataset shared-task 2020\n",
        "test_mednorm = mednorm(testing_df,Train=False) \n",
        "test_mednorm_df = pd.DataFrame.from_records(test_mednorm)\n",
        "test_mednorm_df.columns = ['tweet_id','tweet','label']\n",
        "print('test_mednorm_df:',len(test_mednorm_df))\n",
        "print(test_mednorm_df['label'].value_counts())\n",
        "test_medword_df=testing_df[testing_df['tweet'].str.contains(\"drug|med|medication\")]\n",
        "print('test_medword_df',len(test_medword_df))\n",
        "print(test_medword_df['label'].value_counts())\n",
        "test_filter_df = pd.concat([test_mednorm_df, test_medword_df]).drop_duplicates(keep='first')\n",
        "print('test_filter_df',len(test_filter_df))\n",
        "print(test_filter_df['label'].value_counts())\n",
        "test_lelf_df=pd.concat([testing_df, test_filter_df]).drop_duplicates(keep=False)\n",
        "test_lelf_df.columns = ['tweet_id','tweet','Class']\n",
        "print('test_lelf_df',len(test_lelf_df))\n",
        "print(test_lelf_df['Class'].value_counts())\n",
        "\n",
        "def prob_predict(model,data,output=False):\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "    probs = []\n",
        "    batch_count = 0\n",
        "    for token_tensor, _, attention_mask in data:\n",
        "        with torch.no_grad():\n",
        "            logits = model(token_tensor,token_type_ids=None,attention_mask=attention_mask)[0]\n",
        "        tmp_preds = F.softmax(logits, -1)\n",
        "        tmp_preds = tmp_preds.detach().cpu().numpy()\n",
        "        one_prob = [each[1] for each in tmp_preds]\n",
        "        probs +=  one_prob            \n",
        "    print(len(probs))\n",
        "    return probs\n",
        "\n",
        "from transformers import BertTokenizer,BertForSequenceClassification,AdamW,BertConfig,get_linear_schedule_with_warmup\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "probability = []\n",
        "for i in range(10):\n",
        "  ### load pre-train data to model_1, make sure all the files are in same folder, i.e vocab , config and bin file\n",
        "  model = BertForSequenceClassification.from_pretrained('gdrive/My Drive/Colab Notebooks/bert-large-v3/'+'model '+str(i+1)+'/', cache_dir=None)\n",
        "  ### define def preprocess_1 and load data to def\n",
        "  encoded_tokens_Test,labels_Test,attention_mask_Test = preprocess(test_filter_df)\n",
        "  encoded_labels_Train,encoded_labels_Test,indices_to_label,label_size = label_indexing(labels_Test,labels_Test)\n",
        "  data_test = feed_generator(encoded_tokens_Test, encoded_labels_Test, attention_mask_Test,Training=False,output=True)\n",
        "  preds_test = prob_predict(model,data_test)\n",
        "  probability.append(preds_test)\n",
        "  i += 1\n",
        "\n",
        "avg_probability = np.average(np.array(probability),axis=0)\n",
        "#print(len(avg_probability))\n",
        "avg_predicted_label = []\n",
        "for prob in avg_probability:\n",
        "  if prob > 0.5:\n",
        "    avg_predicted_label.append(1)\n",
        "  else:\n",
        "    avg_predicted_label.append(0)\n",
        "#print(len(avg_predicted_label))\n",
        "print('Testing results of avg ensemble: ')\n",
        "evaluate(avg_predicted_label,test_filter_df['label'],indices_to_label,metric='f1')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13853\n",
            "0    13818\n",
            "1       35\n",
            "Name: label, dtype: int64\n",
            "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linking_model/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/tmpwm8yil4t\n",
            "Finished download, copying /tmp/tmpwm8yil4t to cache at /root/.scispacy/datasets/ea855fd121a193f03190a91417c209d4cd97e63d3ce4b456c248ef7c13a4ca77.03518aabd12de2103a27a50302f37c3d87b0f313a8be08b5ec306c9c4334b9b1.tfidf_vectors_sparse.npz\n",
            "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linking_model/nmslib_index.bin not found in cache, downloading to /tmp/tmpc13416pb\n",
            "Finished download, copying /tmp/tmpc13416pb to cache at /root/.scispacy/datasets/5f620d1bd549a98c005ed601a73806ea2cd1a86ae6c54bbc62bcb3b452ca2630.27a7ac6807fde6628311ff7d70b86fefc640d0eb70637b544c591722a2c16c2a.nmslib_index.bin\n",
            "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linking_model/tfidf_vectorizer.joblib not found in cache, downloading to /tmp/tmpgsbp78ol\n",
            "Finished download, copying /tmp/tmpgsbp78ol to cache at /root/.scispacy/datasets/ffb7a77cdcb3c9233c1e4009c69f38efc13c3619238c508459b5f03fd5f14e4b.9e501319abafbe723d25f1737402d504af6f198fb43b96c85692fa0f9ddc5516.tfidf_vectorizer.joblib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linking_model/concept_aliases.json not found in cache, downloading to /tmp/tmpjp_l3sqg\n",
            "Finished download, copying /tmp/tmpjp_l3sqg to cache at /root/.scispacy/datasets/0f064d20aefab965d5772b2100f8436b3541e7d5313c76cfe5fe070902f149fe.31df9cdb04729860a81bd6c980224ed2bff582586c398d0c9b96ae4e257b9da2.concept_aliases.json\n",
            "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/umls_2017_aa_cat0129.json not found in cache, downloading to /tmp/tmpbxec9bbp\n",
            "Finished download, copying /tmp/tmpbxec9bbp to cache at /root/.scispacy/datasets/13b30cd31cd37c1b52f3df6ea023061172d16e9941660e677fdbb29489af7410.4ad71d86ce780e00cab131c7e3b81acfd2f11dd80ccd61125c8bcde506f2ab8a.umls_2017_aa_cat0129.json\n",
            "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/umls_semantic_type_tree.tsv not found in cache, downloading to /tmp/tmpaet1cfe9\n",
            "Finished download, copying /tmp/tmpaet1cfe9 to cache at /root/.scispacy/datasets/21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv\n",
            "9079\n",
            "0    9044\n",
            "1      35\n",
            "Name: label, dtype: int64\n",
            "202\n",
            "0    195\n",
            "1      7\n",
            "Name: label, dtype: int64\n",
            "9092\n",
            "0    9057\n",
            "1      35\n",
            "Name: label, dtype: int64\n",
            "4761\n",
            "0    4761\n",
            "Name: Class, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:210: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "Testing results of avg ensemble: \n",
            "TP:27, TN:9053, FP:4, FN:8, F1:81.82, Precision:87.10, Recall:77.14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81.81818181818183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eeTLVhS1kJ6"
      },
      "source": [
        "train_addition_lelf=pd.concat([train_addition_df,train_addition_filter_df]).drop_duplicates(keep=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ8xX87dcc54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "c23d9537-835b-498e-a668-40d5e8f2f981"
      },
      "source": [
        "print(train_addition_df.drop_duplicates(keep='first'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  tweet label\n",
            "0     these new anti-anxiety meds make me so sleepy ...     1\n",
            "1     you can try vitamins for olivia ! hudson has t...     1\n",
            "2     toby set up the humidifier for me in the room ...     1\n",
            "3     i am an 8 . they also just started me on pitoc...     1\n",
            "4     i did not take prenatals at all , people did n...     1\n",
            "...                                                 ...   ...\n",
            "9617  i have been up for 3 days , adderal amd redbul...     1\n",
            "9618                        antibacterial sandy wipes !     0\n",
            "9619  we must adopt all of them ! especially the jer...     0\n",
            "9620  nuvaring is not an option for me while breast ...     1\n",
            "9621          rt ibuprofen for breakfast ? runner probs     1\n",
            "\n",
            "[60134 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ntgRnqy4b5A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "66a588d9-5888-4e3d-80a4-c10007521306"
      },
      "source": [
        "test_filter_df['predict_label']=avg_predicted_label[:9092]\n",
        "test_filter_df['prob']=avg_probability[:9092]\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "test_filter_df[test_filter_df['label']=='1'].sort_values(by=['prob'],ascending=True)[0:7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>predict_label</th>\n",
              "      <th>prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4697</th>\n",
              "      <td>596665693874982913</td>\n",
              "      <td>no first thing in the morning and still have this migraine ! ! ! it is been now 4 days and nothing is working ! including a blood patch</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.011236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2749</th>\n",
              "      <td>762059592377458688</td>\n",
              "      <td>the ac can dry you out . get a saline spray . helps keep it from super drying .</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.110055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7644</th>\n",
              "      <td>407420810555445248</td>\n",
              "      <td>yeah im allergic to codiene *idunno if i spelled that right* i learned that the hard way</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.116394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6545</th>\n",
              "      <td>751262835649310722</td>\n",
              "      <td>my friend asked me where she can go to get birth control and i was telling jay about it and he goes god you sound like a mom</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.280192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5316</th>\n",
              "      <td>789840246141755392</td>\n",
              "      <td>but i got my iud just in case lmfao rosie's going to be an only child</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.457933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5277</th>\n",
              "      <td>798626411560910848</td>\n",
              "      <td>so i am just about to drink castor oil again because at this point i am tired</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.689547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5643</th>\n",
              "      <td>407566165473386496</td>\n",
              "      <td>are you good about taking ? health yourself and try out a supplement recommender to find your best bets !</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.791669</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                tweet_id  ...      prob\n",
              "4697  596665693874982913  ...  0.011236\n",
              "2749  762059592377458688  ...  0.110055\n",
              "7644  407420810555445248  ...  0.116394\n",
              "6545  751262835649310722  ...  0.280192\n",
              "5316  789840246141755392  ...  0.457933\n",
              "5277  798626411560910848  ...  0.689547\n",
              "5643  407566165473386496  ...  0.791669\n",
              "\n",
              "[7 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yop7-qlQFZe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345,
          "referenced_widgets": [
            "e88676fe811648a8b9e905e6af6ddaa9",
            "8ac7037a08ff491c8e9c027d604f93e4",
            "bcc5f758eb2c41c3aed8031d37f62857",
            "f0ad1803b69646c7a93aafb1fbf8c01f",
            "bd98e7acd29e4a42b6a702fb3e53c6d7",
            "9ecad717627e4d31a61f51b5b2574f77",
            "fc27c4eedc424853a96903db3c781e06",
            "b7a20779d0e446acbd2e825c3ca6f263",
            "cff0de2165014919a40c638dda1a133a",
            "af9344008f564c34a951cba1e5adddc9",
            "c3d6c79a348e47c0b34d2b06285a8c71",
            "5d7b51f3b74148998e5b1270c2e0d860",
            "d6753e0925ed4ab691d513ed07bb8d42",
            "67daa58fc630407eb689ae79bced7db6",
            "0e31b48895e742b8b57e1b646706282e",
            "ce53e18e23fd42fe814216e95e7d806a"
          ]
        },
        "outputId": "5d4a583b-8e97-40b7-f498-62d1d97d586d"
      },
      "source": [
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification,AdamW,BertConfig,get_linear_schedule_with_warmup\n",
        "tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
        "probability_1 = []\n",
        "for i in range(10):\n",
        "  ### load pre-train data to model_1, make sure all the files are in same folder, i.e vocab , config and bin file\n",
        "  model_1 = AutoModelForSequenceClassification.from_pretrained('gdrive/My Drive/Colab Notebooks/bio-clinical-bert-v3/'+'model '+str(i+1)+'/', cache_dir=None)\n",
        "  ### define def preprocess_1 and load data to def\n",
        "  encoded_tokens_Test_1,labels_Test_1,attention_mask_Test_1 = preprocess(test_filter_df)\n",
        "  encoded_labels_Train_1,encoded_labels_Test_1,indices_to_label_1,label_size_1 = label_indexing(labels_Test_1,labels_Test_1)\n",
        "  data_test_1 = feed_generator(encoded_tokens_Test_1, encoded_labels_Test_1, attention_mask_Test_1,Training=False,output=True)\n",
        "  preds_test_1 = prob_predict(model_1,data_test_1)\n",
        "  probability_1.append(preds_test_1)\n",
        "  i += 1\n",
        "\n",
        "avg_probability_1 = np.average(np.array(probability_1),axis=0)\n",
        "\n",
        "avg_predicted_label_1 = []\n",
        "for prob in avg_probability_1:\n",
        "  if prob > 0.5:\n",
        "    avg_predicted_label_1.append(1)\n",
        "  else:\n",
        "    avg_predicted_label_1.append(0)\n",
        "#print(len(avg_predicted_label))\n",
        "print('Testing results of avg ensemble: ')\n",
        "evaluate(avg_predicted_label_1,test_filter_df['label'],indices_to_label,metric='f1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e88676fe811648a8b9e905e6af6ddaa9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=385.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cff0de2165014919a40c638dda1a133a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "Testing results of avg ensemble: \n",
            "TP:26, TN:9056, FP:1, FN:9, F1:83.87, Precision:96.30, Recall:74.29\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83.87096774193549"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tmw6vRkGB5J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "d186302a-bf08-467c-dbe0-8438bbaf6cf2"
      },
      "source": [
        "test_filter_df['predict_label']=avg_predicted_label_2[:10355]\n",
        "test_filter_df['prob']=avg_probability_2[:10355]\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "test_filter_df[test_filter_df['label']=='0'].sort_values(by=['prob'],ascending=False)[0:7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>predict_label</th>\n",
              "      <th>prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3110</th>\n",
              "      <td>466014744949030912</td>\n",
              "      <td>benzino face strong as shit</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.887632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1479</th>\n",
              "      <td>879783369403510784</td>\n",
              "      <td>i was using medela bm storage bags bc you know they are medela and they are not going to leak . i ran out of them and started using the lansinoh</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.795405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>502548414362841091</td>\n",
              "      <td>cayden's already starting to open his eyes ! the anesthesia might wear off sooner than we thought ,</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.645316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>799809931436118016</td>\n",
              "      <td>i wet the pad with the peri bottle and it took away the burn bc it soothed my swelling and the witch hazel pads helped too !</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.567676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2335</th>\n",
              "      <td>840809281867919361</td>\n",
              "      <td>dbn cops can teach sandton a thing or2 who still cant arrest a maxitaxi driver even with footage and all .</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.550652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3391</th>\n",
              "      <td>443166656651468800</td>\n",
              "      <td>sometimes when i am nauseated i eat a few peperoncinis and then i feel better .</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.532981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6540</th>\n",
              "      <td>879728512101429250</td>\n",
              "      <td>i struggle with my dog sleeping with me but brinyn stayed last night so it was ten times worse lol .</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.520060</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                tweet_id  ...      prob\n",
              "3110  466014744949030912  ...  0.887632\n",
              "1479  879783369403510784  ...  0.795405\n",
              "0     502548414362841091  ...  0.645316\n",
              "59    799809931436118016  ...  0.567676\n",
              "2335  840809281867919361  ...  0.550652\n",
              "3391  443166656651468800  ...  0.532981\n",
              "6540  879728512101429250  ...  0.520060\n",
              "\n",
              "[7 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wfuJVnENlB2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "06a4dcea208249fd9c0ce964e1716812",
            "2943f6352a804a3c86b456ca2780a3d0",
            "7a1ad279a59648fbaea1c973fe7edad8",
            "c614654b86814e98a56d6452b18ddb02",
            "8c1094065d354031aeb43a2ab2f67bb8",
            "9cc1ab2c34574d2e86ab1abb51be3f54",
            "08a8bc2a39c14948b9b8e5d63496e7ae",
            "7df19be9f01443698c84e4d3241ca8cd"
          ]
        },
        "outputId": "a28af56b-7112-4ece-e7a1-2d16783e8939"
      },
      "source": [
        "from transformers import BertTokenizer,BertForSequenceClassification,AdamW,BertConfig,get_linear_schedule_with_warmup\n",
        "probability_2 = []\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "for i in range(10):\n",
        "  ### load pre-train data to model_1, make sure all the files are in same folder, i.e vocab , config and bin file\n",
        "  model_2 = BertForSequenceClassification.from_pretrained('gdrive/My Drive/Colab Notebooks/bert-base-v3/'+'model '+str(i+1)+'/', cache_dir=None)\n",
        "  ### define def preprocess_1 and load data to def\n",
        "  encoded_tokens_Test_2,labels_Test_2,attention_mask_Test_2 = preprocess(test_filter_df)\n",
        "  encoded_labels_Train_2,encoded_labels_Test_2,indices_to_label_2,label_size_2 = label_indexing(labels_Test_2,labels_Test_2)\n",
        "  data_test_2 = feed_generator(encoded_tokens_Test_2, encoded_labels_Test_2, attention_mask_Test_2,Training=False,output=True)\n",
        "  preds_test_2 = prob_predict(model_2,data_test_2)\n",
        "  probability_2.append(preds_test_2)\n",
        "  i += 1\n",
        "\n",
        "avg_probability_2 = np.average(np.array(probability_2),axis=0)\n",
        "\n",
        "avg_predicted_label_2 = []\n",
        "for prob in avg_probability_2:\n",
        "  if prob > 0.5:\n",
        "    avg_predicted_label_2.append(1)\n",
        "  else:\n",
        "    avg_predicted_label_2.append(0)\n",
        "#print(len(avg_predicted_label))\n",
        "print('Testing results of avg ensemble: ')\n",
        "evaluate(avg_predicted_label_2,test_filter_df['label'],indices_to_label_2,metric='f1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06a4dcea208249fd9c0ce964e1716812",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "9096\n",
            "Testing results of avg ensemble: \n",
            "TP:26, TN:9056, FP:1, FN:9, F1:83.87, Precision:96.30, Recall:74.29\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83.87096774193549"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7PPKhwgFo1k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0aadaa97-17c9-4d3a-dfc1-733680b12600"
      },
      "source": [
        "avg_probability_ensemble = np.average(np.array([avg_probability,avg_probability_1,avg_probability_2]),axis=0,weights=[1/3,1/3, 1/3])\n",
        "\n",
        "#print(len(avg_probability))\n",
        "avg_predicted_ensemble_label = []\n",
        "for prob in avg_probability_ensemble:\n",
        "  if prob > 0.5:\n",
        "    avg_predicted_ensemble_label.append(1)\n",
        "  else:\n",
        "    avg_predicted_ensemble_label.append(0)\n",
        "#print(len(avg_predicted_label))\n",
        "print('Testing results of ensemble: ')\n",
        "evaluate(avg_predicted_ensemble_label[:10385],test_filter_df['label'],indices_to_label,metric='f1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing results of ensemble: \n",
            "TP:26, TN:9055, FP:2, FN:9, F1:82.54, Precision:92.86, Recall:74.29\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82.53968253968254"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsV-g6Qrcv8h"
      },
      "source": [
        "#del test_lelf_df['tweet']\n",
        "#del test_filter_df['tweet']\n",
        "#del test_filter_df['label']\n",
        "test_filter_df['Class']=avg_predicted_ensemble_label[:9092]\n",
        "predict_test_df = pd.concat([test_filter_df, test_lelf_df])\n",
        "#print(test_lelf_df)\n",
        "#print(test_filter_df['label'].value_counts())\n",
        "\n",
        "\n",
        "\n",
        "#predict_test_df.options.display.float_format = '{:.0f}'.format\n",
        "\n",
        "#for i, value in enumerate(predict_test_df['tweet_id']):\n",
        "#  predict_test_df['tweet_id'][i] = predict_test_df['tweet_id'][i].to_int()\n",
        "predict_test_df['tweet_id'] = predict_test_df['tweet_id'].astype(str)\n",
        "predict_test_df['Class'] = predict_test_df['Class'].astype(str)\n",
        "test_df['tweet_id']=test_df['tweet_id'].astype(str)\n",
        "del test_df['tweet']\n",
        "del test_df['label']\n",
        "test_df.merge(predict_test_df, left_on='tweet_id', right_on='tweet_id').to_csv('gdrive/My Drive/Colab Notebooks/prediction_task1_1.tsv',index=False,sep='\\t', line_terminator = '\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgS9YoE1HFNU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "0cbfb677-bcbb-4963-b299-97bf44a0bba4"
      },
      "source": [
        "test_filter_df['predict_label']=avg_predicted_ensemble_label[:10355]\n",
        "test_filter_df['prob']=avg_probability_ensemble[:10355]\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "test_filter_df[test_filter_df['label']=='0'].sort_values(by=['prob'],ascending=False)[:11]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>predict_label</th>\n",
              "      <th>prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1479</th>\n",
              "      <td>879783369403510784</td>\n",
              "      <td>i was using medela bm storage bags bc you know they are medela and they are not going to leak . i ran out of them and started using the lansinoh</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.745258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>502548414362841091</td>\n",
              "      <td>cayden's already starting to open his eyes ! the anesthesia might wear off sooner than we thought ,</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.725964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3110</th>\n",
              "      <td>466014744949030912</td>\n",
              "      <td>benzino face strong as shit</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.448109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9111</th>\n",
              "      <td>749930523057803265</td>\n",
              "      <td>these lil minors jus walked up on me and asked me can i buy their rellos . i do not know if i am mad that i helped condone that shit or . . .</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.376362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4801</th>\n",
              "      <td>783596946564976640</td>\n",
              "      <td>stop using echofon</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.373988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2098</th>\n",
              "      <td>625283798343766016</td>\n",
              "      <td>yeah . i took their wretched pills and syrups and did it all anyways</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.361622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8882</th>\n",
              "      <td>417361586949353472</td>\n",
              "      <td>lucky , mine are like soft gel capsules . i heard they have gummi ones . deff getting those next lol .</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.355874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3897</th>\n",
              "      <td>660213592466268160</td>\n",
              "      <td>what sort of equip . are you using for digital illustration ? i am interested in anything but tablets ( hand/eye coordination drives me nuts ! )</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.353218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1407</th>\n",
              "      <td>831263469840302081</td>\n",
              "      <td>have you tried viva vegeria ? it is on nogalitos by the heb .</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.304541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4682</th>\n",
              "      <td>782937098093486080</td>\n",
              "      <td>oh no , the 'make children paranoid they are being constantly spied on by putting a creepy figurine on a shelf until xmas' has begun . . .</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.292895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>799809931436118016</td>\n",
              "      <td>i wet the pad with the peri bottle and it took away the burn bc it soothed my swelling and the witch hazel pads helped too !</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.288851</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                tweet_id  ...      prob\n",
              "1479  879783369403510784  ...  0.745258\n",
              "0     502548414362841091  ...  0.725964\n",
              "3110  466014744949030912  ...  0.448109\n",
              "9111  749930523057803265  ...  0.376362\n",
              "4801  783596946564976640  ...  0.373988\n",
              "2098  625283798343766016  ...  0.361622\n",
              "8882  417361586949353472  ...  0.355874\n",
              "3897  660213592466268160  ...  0.353218\n",
              "1407  831263469840302081  ...  0.304541\n",
              "4682  782937098093486080  ...  0.292895\n",
              "59    799809931436118016  ...  0.288851\n",
              "\n",
              "[11 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mscjte3doonc"
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow import summary"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}